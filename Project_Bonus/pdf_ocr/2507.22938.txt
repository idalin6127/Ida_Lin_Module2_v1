

--- Page 1 ---

A Graph-based Approach for Multi-Modal Question Answering
from Flowcharts in Telecom Documents
Sumit Soman, H. G. Ranjani, Sujoy Roychowdhury, Venkata Dharma Surya Narayana Sastry, Akshat
Jain, Pranav Gangrade, Ayaaz Khan
{sumit.soman, ranjani.h.g,sujoy.roychowdhury}@ericsson.com
Ericsson R&D
Bangalore, Karnataka, India
Abstract 1 Introduction
Question-Answering (QA) from technical documents often involves With advances in Large Language Models (LLMs), Retrieval Aug-
Va) questions whose answers are present in figures, such as flowcharts mented Generation (RAG) based Question-Answering (QA) sys-
N or flow diagrams. Text-based Retrieval Augmented Generation tems for technical domains like telecom have become prevalent
=p) (RAG) systems may fail to answer such questions. We leverage [11, 12, 15], which leverage domain-adapted models [5, 13]. How-
N graph representations of flowcharts obtained from Visual large Lan- ever, these works largely focus on text-based input and output
— guage Models (VLMs) and incorporate them in a text-based RAG modalities for RAG based QA task. A typical telecom engineer
5 system to show that this approach can enable image retrieval for QA would require to refer to information present in Product Informa-
om) in the telecom domain. We present the end-to-end approach from tion Documents (referred to as PI Docs in this work) and assimilate
P PP
LV ay processing technical documents, classifying image types, building information from images (or figures), tables and/or equations, apart
N graph representations, and incorporating them with the text embed- from text, to provide answers to user queries, or for troubleshooting
ding pipeline for efficient retrieval. We benchmark the same on aQA scenarios.
) dataset created based on proprietary telecom product information
documents. Results show that the graph representations obtained 1.1 Background
S) using a fine-tuned VLM model have lower edit distance with re- In the literat Iti-modali er nt d
N spect to the ground truth, which illustrate the robustness of these n h Werarure, MY le ed ‘L, ity i. viene i nM WOAY woke the
oO representations for flowchart images. Further, the approach for QA wit text are typically addressed as Visual QA (VQA) tasks t at
Ld : : : : : use Visual Language Models (VLMs) [6]. However, models trained
using these representations gives good retrieval performance using . : . :
. . . . on public datasets, which may include images of cat, dog, or back-
— text-based embedding models, including a telecom-domain adapted ; : :
> one. Our approach also alleviates the need for a VLM in inference, ground objects, are not useful for domains such as telecom. Typical
le) which is an important cost benefit for deployed QA systems images in telecom-domain documents pertain to categories such as
a P Ploy y , screenshots of tools, flowcharts, block diagrams, sequence diagrams,
O icons, equations, schematic diagrams, among others, as can be seen
N CCS Concepts in [1]. It is evident that images here often contain information
N - Computing methodologies — Natural language processing; primarily as text, and may also include icons, schematic represen-
~ - Applied computing — Document searching. tations, connections and dependencies among connected blocks or
>) objects. The textual content is dominant and also domain-intensive.
Vay Keywords Popular RAG systems either support text modality alone or use
Iti-modal embeddings i tor database t ti yi
N Multi-Modal QA, Flowchart Representations, Retrieval Augmented MU MO GAY EMDBECAMES VECTOR Cakavase FO SUPPort Mages, i
ee Generation. RAG. Large Lansuage Models. LLM. VLM. Telecom addition to text in the input. However, the cost of using VLMs for
. = ; ars suas ; ; ; embedding, and as the generator for RAG in an inference setup,
< ACM Reference Format: is formidable for business needs due to the large size of VLMs [8].
oan Sumit Soman, H. G. Ranjani, Sujoy Roychowdhury, Venkata Dharma Surya App roaches using knowledge grap hs [3], as is common with textual
) Narayana Sastry, Akshat Jain, Pranav Gangrade, Ayaaz Khan. 2025. A Graph- data, require manual verification and hence are not suitable. :
based Approach for Multi-Modal Question Answering from Flowcharts in In this work, we focus on parsing, categorizing and processing
Telecom Documents. In Proceedings of KDD ’25 Workshop on Structured images from proprietary technical documentation in the telecom
Knowledge for LLMs. ACM, New York, NY, USA, 10 pages. domain. We specifically focus on flowchart images which are hard
to interpret. These flowcharts present a unique issue of having
mostly text-based content and are also indicative of decision rules
Permission to make digital or hard copies of all or part of this work for personal or and conditions. These, in turn, are a very valuable source of infor-
classroom use is granted without fee provided that copies are not made or distributed mation, especially in configuration and troubleshooting of telecom
for profit or commercial advantage and that copies bear this notice and the full citation networks. However, summarizing a flowchart as text via VLMs
on the first page. Copyrights for components of this work owned by others than the . . ; . . . .
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or might be tedious, prone to hallucinations, difficult to quantify ac-
republish, to post on servers or to redistribute to lists, requires prior specific permission curacy, and potentially may lose out on various conditions (refer
and/or a fee. Request permissions from permissions@acm org. Figure 1). Converting flowcharts to graph structures has been ad-
KDD ’25 Workshop on Structured Knowledge for LLMs, Toronto, CA d din th li ks of H h k
© 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM. ressed in the earlier works of [10, 14]. However, these works are
limited to evaluating the conversion of flowcharts to graphs and


--- Page 2 ---

KDD ’25 Workshop on Structured Knowledge for LLMs, August 04, 2025, Toronto, CA Soman et al.
Response: This image is a flowchart or process diagram that visually represents a series of interconnected elements through labeled shapes and
directional arrows. Below is a detailed description:

### “General Layout**
1. The diagram begins at the top with a circular shape labeled **"quaketail storywise"**, which appears to be the starting point or central concept.
2. From this initial circle, various rectangular, trapezoidal, and diamond shapes are connected by arrows, indicating relationships or sequential steps.
3. The arrows flow in different directions, showing a non-linear progression, with some elements linking back to earlier nodes.
### “Key Components**
1. **Central Node:**
quaketall storywise - The circle labeled **"quaketail storywise"** is likely the main focus or origin point of the diagram.
- It branches into two distinct paths:
- **Left Path:** Leads to "epopoeias besoms."
ee - “Right Path:** Leads to “nonnaturalistic."
; — 2. **Left Path:**
GeapeESECEGis nonnaturalistic |  -"epopoeias besoms" connects to "homosporous" through two arrows.
U t _F C - From “homosporous," the flow moves downward to **"KP@TwiXF'"** and ultimately to a diamond-shaped node labeled **"grummet gaudete"**.
— a | 3."*Right Path:**
INEATEEEARS carnalized ims) | =“nonnaturalistic’ connects to "carnalized."
; 4 } 7 Another arrow from “nonnaturalistic” leads downward to **"jMMnSj5"**, which in turn connects to **"cardholder."**
———— } 7 “carnalized" connects to "jelerang introductor," which also leads to "grummet gaudete.”
| KPOTWiXF jelerang introductor | 4. **Diamond Node:**
| | _ -The diamond-shaped node **"grummet gaudete"** acts as a junction where multiple paths converge.
\ | - It flows downward into two separate paths:
VN | - **"wronghearted shrave"**
- Apath that loops back upward, connecting to “cardholder.”
SRST SS EST _cerchelser | ### Connections and Relationships**
| ~The diagram includes multiple feedback loops, where nodes like "cardholder" reconnect to earlier nodes in the flow, such as “wronghearted shrave" and
_— “jelerang introductor.”
| _ _ J | 7 The layout suggests a system with a mix of linear and cyclical relationships, potentially representing processes or dependencies.
ro - ### **Visual Design**
wronghearted shrave - Shapes:
- Circular node: Represents the central concept.
- Rectangles and trapezoids: Represent intermediate or secondary ideas.
- Diamond: Represents a decision point or convergence of multiple flows.
- Arrows: Represent directional flow or relationships between elements.
Prompt: Provide a detailed description of - Colors: All elements are shaded in light purple, with a consistent style used throughout.
uploaded image -

Figure 1: A sample description of synthetic flowchart from Flowlearn using GPT4 Vision model. The incorrect part of description

are highlighted in gray. This illustrates the issues faced in using VLMs directly for technical QA involving flowcharts.

evaluating the best representation of flowcharts (graphs or UML) e Automatically categorize images parsed from technical doc-
for QA. The latter work, in fact, assumes that the correct flowchart umentation using fine-tuned Document Image Transformer

is available for answering the question. Hence, there exists a gap (DIT) model [7].

in the study of retrieving the right flowchart or its representation. e Convert flowchart images to graph structures using a fine-

We build on some of these existing works to propose a solution tuned VLM.

which uses fine-tuned VLMs to convert flowcharts’ images to graph e Evaluate various chunking approaches to introduce these

representations, and benchmark it for retrieval for a typical QA graphs in vector database for retrieval.

task based on these flowcharts. e Benchmark retrieval performance on QA dataset based on

flowcharts.

1.2 Problem Statement The rest of the paper is organized as follows: Section 2 details

We propose the enhancement of text-only RAG with the following the proposed approach, with details of image classification (Section

capabilities: 2.1), conversion of flowcharts to graphs (Section 2.2), chunking

. . and ingestion (Section 2.3) and evaluation (Section 2.4). Experimen-
e Categorize parsed images (from telecom documents - PI . : . .
oo , . tal setup details are provided in Section 3 followed by the results
Docs) into image categories identified as relevant for telecom os . . .
domain QA and analysis in Section 4. Finally, conclusions and future work are
omain QA.
. resented in Section 5.
e Use a fine-tuned VLM to convert flowchart images to graph P
structures, using nodes connected through edges (unidirec-
tional and/or bidirectional). The nodes and edges also have 2 Pr oposed Appr oach
attributes associated. As mentioned earlier, QA on flowcharts can be challenging using
e Jointly represent flowchart based graph structures inter- VLMs directly (since they may comprise information related to
spersed with text using LM domain-adapted embeddings. flow of information between nodes, decisions based on conditions
¢ Utilize the graphical structures in RAG pipeline for improved or sequence of steps). Evaluation of these textual description of
coverage during retrieval. flowcharts can be challenging as there is no ground truth available
and these descriptions can tend to be verbose based on complexity
. . of flowcharts (refer Fig 1).
1.3. Contributions Our proposed approach entails classification of images into vari-
The contributions of this work are as follows: ous categories. Next, we consider only flowchart images. We use


--- Page 3 ---

Graph Representations for Flowchart QA KDD ’25 Workshop on Structured Knowledge for LLMs, August 04, 2025, Toronto, CA
VLM model
Ree nTec Map to JSON parsed from document Metacer heading
(Section 2.2) - Image and table captions
Document Processor = es a
Image Type Classifier ‘| GuiEwe ]
- Images, captions ; “yeneration Vector (eae Response
; = Store sig LM |
! ee
- Pipeseparatedvalues ! !
- Column header prefixed User query in !
for each entry I the prompt I
! eo !
|
; Respons ;
, inference 1
Figure 2: The end-to-end approach for multi-modal QA, our approach for images is indicated in black-shaded boxes.
fine-tuned VLMs (trained using publicly available database) to con- edges, and any text on the link is considered as an edge attribute.
vert these domain-specific flowchart images to graph represen- Although flowcharts may have different shapes of blocks, we do
tations. In the subsequent step, we use text-based chunking and not capture that in the node information. A sample representation
obtain embeddings of these for retrieval. This section details the pro- of a flowchart and the corresponding graph (in JSON format) is
posed approach and evaluation metrics. Fig. 2 depicts the proposed shown in Figs. 3a and 3b, respectively.
end-to-end approach for multi-modal QA.
2.2.1 Using a fine-tuned VLM. In order to improve the graph repre-
2.1 Classification of images sentation generated by the open source VLM, we also fine-tuned a
open-source VLM using the publicly available synthetic flowcharts
Telecom documentation can have various categories of images. Typ- of Flowlearn dataset [10].
ically, PI Docs processing involves parsing various formats (such It may be noted here that the model was fine-tuned on a publicly
as HTML, PDF) of documents and extracting the text, tables, equa- available dataset of flowcharts and used to generate graphs for
tions and images from the paragraphs of various sections. Typically, telecom-domain flowcharts, as shown in Fig. 4. Details of VLM
the textual components are chunked (optimally) converted to em- considered and training dataset are given in Section 3.
bedding vector using domain-adapted embedding model [16] and
ingested into a vector database. In our approach, we aim to in- 2.2.2 Evaluation metric for VLM. The output of the VLM isa graph
troduce only flowcharts using textual embedding for retrieval. To representations of the flowchart. Hence, we use Graph Edit Distance
achieve this, we train a classifier to categorize the images parsed (GED) [2] as a measure of the performance of the model, apart from
from PI Docs into various categories, viz., block diagrams, equa- number of nodes and edges accurately detected. A lower value of
tions, flowcharts, graphs, hardware diagrams, icons (navigation, GED indicates close similarity of the generated graph representa-
logos), schematic diagrams, screenshots, sequence diagrams. This tion when compared with the ground-truth representation of that
helps categorize images for subsequent downstream QA task. In flowchart. We show in our results that we obtain lower GED with
this work, we use the flowchart images for further processing and the fine-tuned VLMs.
obtain their corresponding graph representation.
2.3 Chunking, ingestion into vector store and
2.2 Graph representation for flowcharts retrieval
We filter on the images identified as flowcharts from the classifier The graph structures obtained from the previous step (detailed in
model discussed in Section 2.1. On these, we propose to use a fine- Section 2.2) must be introduced into the vector store to ensure these
tuned VLM to generate graph representations of the images. are included in the retriever stage of RAG. Hence, it is essential
A flowchart consists of a number of blocks and interconnecting to be able to obtain embedding vectors for these graph structures.
links between them. We create a directed graph out of the flowchart There have been studies which perform experiments to find optimal
- we represent each block as a node and capture the text within the chunking of textual data and for tables to improve retrieval accu-
block as a node attribute. Links between blocks are considered as racy [16]. Similarly, it is of importance to understand the optimal


--- Page 4 ---

KDD ’25 Workshop on Structured Knowledge for LLMs, August 04, 2025, Toronto, CA Soman et al.
{
“nodes”: [
{ {"edges": [
"id": "start", {
“label”: "Start" “from: "start ;
i "to": "xyz1",
“label”: "xyz" L "from": "condition2",
i { ‘ "to": “def",
, 7 "from": "xyz", "value": “Yes"
XyZ "id': “condition1", "to" “condition 1"
sabef" "Condition?" "to": "condition1”, 1
} value": {
{ i "from": "condition2",
"id": “abc", "to": "par",
Condition Yes abc "label": “abc" “from”: "value": "No"
rf t "condition1", },
"id": “condition2", "to": “abe", . { "
"label": "Condition2 ?" value’: “Yes "from": “def",
h h "to": “pqr",
Condit { het { ' "value": “"
‘ondition "id": "def", "from":
a “label”: “def “condition1", t
i "to": "condition2", "from": "par",
“id": "par", "value": “No" "to": "stop",
"label": “par } "value": “edge8"
L { }
{ "from": “abc", ]
"id": "stop", "to": "def", }
“label”: "Stop" "value": “"
}
(a) A sample image of a flowchart. (b) Graph representation in JSON (JavaScript Object Notation) with
nodes and edges.
Figure 3: A sample flowchart and its graph representation in JSON format.
TVLM Fine-tuning | Coen) [Chunking Strategies |
] lowlearn ves
| eee VLM Fine-Tuned I <> a | YT
Model —_ | | ee
Se eee eee eS — —_ reo V—— 4 Cs) iss
a Graph JSON Generation . .
Telecom Flowchart Figure 5: Chunking approaches for flowchart JSONs.
Dataset
| i |
! SS
@ 1 ; 1 2.4 Evaluation of Retrieval for RAG
Embeddings Vector 1
a 1 generation St , We evaluate retrieval using two embedding models, the bge-large
nf ore , [18] which is a publicly available embedding model and the TeleR-
_nerence 5 ----__ Kee oBERTa [4], a telecom-domain adapted embedding model. For evalu-
ating the retrieval, we compute top-k accuracy for k = {1, 3, 5}. Fol-
Figure 4: Fine-tuning the VLM with a publicly available lowing is the evaluation criteria adopted for the respective chunking
dataset and using it on telecom data for generating graph approaches:
representations. . . :
P e When textual information of each node is embedded as a
vector, the retrieval is correct if any of the nodes from the
chunking mechanism for embedding graph structures. We consider top-k retrieved graph J SON S appears 1 the ground truth.
the following options to generate embeddings, as shown in Fig. 5: e When textual information of all the nodes is embedded as a
; . single vector, the retrieval is considered correct if all of the
e Each node as one chunk: Embed each node’s textual infor- . .
; : : nodes from the ground truth appear in the top-k retrieved
mation as a single embedding vector.
; graph JSONs.
e All nodes as one chunk: Embed all the node textual infor- . . .
: : . e When the entire textual information in the graph JSON is
mation as a single embedding vector. . . . .
: ; embedded as a single vector, the top-k retrieval is considered
e Entire graph JSON as one chunk: Embed the entire textual .
, . : . correct one of the graph JSON among the k- retrieved graphs
information from graph JSON as a single embedding vector.
corresponds to the ground truth.


--- Page 5 ---

Graph Representations for Flowchart QA KDD ’25 Workshop on Structured Knowledge for LLMs, August 04, 2025, Toronto, CA

3 Experiments (E) and ‘Node related’ (N) - based on how to arrive at the answer

3.1. Datasets from the question. Details of the QA dataset are listed in Table 1 for
different hops. The categories listed are based on ability of retriever

For the dataset creation, we initially considered two sources - (i) to identify the correct chunk of data which contains the graphical

publicly available 3GPP (Rel 18) documents (ii) proprietary PI Docs structure from the flowchart image.

- both of which pertain to the telecom domain.

3.1.1 Image Sources. From the 36 series of the publicly available

3GPP (Rel 18) documents [1], we parse 6342 images comprising of Decision related 359

various categories such as schematic diagrams, graphs, frequency Node related 487

plots, block diagrams, sequence diagrams. However, there are very Edge related 479

few examples of flowcharts (less than 2%) and many of these can Table 1: Distribution of QA categories considered

be considered as a mix of flowchart as well as block diagram (refer

Fig. 9 in Appendix B). Hence, we do not consider the flowcharts

from this source, as it is not representative of the data we encounter

during troubleshooting. .

From the proprietary PI Docs, we have identified few documents 3.2 Experimental Setup

to include in this dataset, extracted 1586 images of multiple types 3.2.1 Image Category Classifier. We use the manually annotated PI

such as block diagrams, equations, flowcharts, graphs, hardware di- Docs image dataset (1586 images) described in Section 3.1 to train

agrams, icons (navigation, logos), schematic diagrams, screenshots, (fine-tune) the image classification model. This dataset is split into

sequence diagrams, among others. Fig. 8 shows sample images for train and test dataset in the ratio 80-20 split (1268-318 train-test

the categories of images seen in PI Docs. Since this is a proprietary split) using stratified sampling. We fine-tune the “microsoft/dit-base”

dataset, all content in images pertaining to PI Docs source have model [7] with batch size of 16 for this dataset.

been obfuscated in this manuscript to retain confidentiality. Table 2 While it is possible to explore models other than DIT, we note

shows the image categories considered and their statistics. We have that image classification is not the primary focus of this work and

manually annotated this dataset to construct training and test sets this model performance can be considered as a baseline for further

with 1268 and 318 images respectively using stratified sampling improvements.

across these image types. This dataset was used to fine-tune the 3.2.2 VLM for graph representation. The top-performing open-

classifier model that predicted the image type. source VLMs ! available at the time of conducting our experiments

3.1.2 Image to graph representation. For fine-tuning, we consider were Qwen2-VL [17] and Llava 1.5 [9]. We considered Qwen2-VL for

synthetic flowcharts component of Flowlearn dataset [10] for train- fine-tuning due to better performance on few samples. The prompt

ing VLM model. This consists of 10,000 flowchart images generated used for generating the graph representation of a flowchart is:

with Mermaid tool. The mermaid tool script is mapped to the re- ‘T have uploaded an image of a flowchart and here is its

quired JSON format and is used for fine-tuning. However, we found ground truth JSON representation, image_json ={}. Now

that this dataset did not consider the following scenarios typically generate JSON for the next image, from and to should

seen in flowcharts: be the node IDs. In the edges section, make sure that

e Shapes of nodes such as rectangles with semicircular ends, the edge value is present. If there are multiple identi-
parallelograms, decision boxes (typically shaped as rhom- cal nodes, create different IDs for them and their edges
bus), connectors (shaped as circles and pentagons) accordingly.”

e Edges, including bi-directional edges, multiple arrow heads The synthetic flowcharts from Flowlearn dataset [10] is consid-
(small, medium, large), various edge connectors (solid, dotted, ered for fine-tuned VLM. It has 10,000 images, split as 64-16-20%
dashed), straight sharp edges for train, validation and test respectively. We augment this training

e Node lines - solid, dotted, dashed and sometimes, no outer set with synthetic data for improved coverage of various nodes and
lines edges (details described in 3.1). The fine-tuning was performed for

e Edge attributes with text associated few choices of parameters R and a, and the best fine-tuned model

In order to deal with such scenarios, we synthetically created images was used in the pipeline.

using Mermaid tool, using existing Flowlearn dataset as the starting 3.2.3 Retrieval with chunking approaches. As detailed in Section 2.4,
point. These synthetically generated images were augmented with we evaluate various chunking approaches via retrieval accuracies.
existing images of Flowlearn dataset. The augmented images for Three chunking approaches are proposed and evaluated for top-k
training (fine-tuning) and testing are kept separate. accuracies. The chunking approaches are evaluated in two scenarios:
3.1.3. QA Dataset for Retrieval. For testing retrieval accuracy, we (i) Embedding vectors of only graph-structures are considered for
consider 105 flowchart images from PI Docs which have been asso- retrieval (ii) Embeddings of grap h-structure and accompanying text
ciated with ground truth graph structures. With inputs from Subject are considered for retrieval. This results in 6 variations of retrieval
Matter Experts (SMEs), we carefully curate a set of 502 QA pairs for each model. We consider two embedding models - bge-large and
from these images, with a mode of ~ 5 questions for each image. 'Since PI Docs are proprietary in nature, it is preferable to use open-source VLMs to
Each QA pairs is tagged as ‘Decision related’ (D), ‘Edge related’ avoid data-sharing outside the organization


--- Page 6 ---

KDD ’25 Workshop on Structured Knowledge for LLMs, August 04, 2025, Toronto, CA Soman et al.
TeleRoBERTa (domain-adapted) [4, 18] and hence will have 12 sets Similar performance evaluation of flowcharts from PI Docs is
of retrieval results. reported using the best performing fine-tuned model in Table 3.
We observe that the average number of nodes and edges in the
4 Results and Analysis flowcharts from PI Docs is almost twice that of those seen in Flow-
In this section, we tabulate and detail results of the experiments chart dataset. Results show that GED on this unseen flowchart data
listed in experimental setup section. is quite low (3.14).
4.1 Image Category Classifier 4.3 Retrieval with chunking approaches
We consider dataset of 1586 images from the proprietary dataset Table 4 shows the retriever performance on top-k accuracy for
and labeled them manually to create training and test datasets. The the chunking approaches using a publicly available base model
distribution of number of images in the respective categories is (bge-large) and domain-adapted (TeleRoBERTa) embedding models.
shown in Table 2. We fine-tuned the “microsoft/dit-base” model [7] We highlight here that the TeleRoBERTa model has been domain-
with batch size of 16 for this dataset. adapted on publicly available telecom data (3GPP), and does not
include any images related information during its training phase.
Hence, there is no data contamination for the data considered and
L Block Diagram 123 39 162 evaluated in this work. The objective here is to compare it? with
2. Equation 154 42 196 a publicly available embedding model. We observe that the best
3. Flowchart 171 38 209 top-k retrieval results for k = {1,3} are obtained when using for
4 Graph 41 9 50 TeleRoBERTa (57.17% and 71.91% respectively). Further, using each
5. Hardware 7 3 10 node as one chunk (embedding vector) gives better results for top-1
6. Icon 15 3 18 (57.17%) for TeleRoBERTa.
7. Others 131 43 174 We also observe that retrieval accuracy reduces in the scenario
8. Schematic Diagram | 168 29 197 when embeddings of graph structures are interspersed with text
9. Screenshot 388 102 490 in the vector store. This is expected, as in a typical scenario, the
10. Sequence Diagram | 70 10 80 retrieved top-k embeddings are no longer limited to only graph
structures.
[| Total | 1208 | 318 1586 Table 5 shows the top-k retrieval accuracy for the embedding
Table 2: Image categories for telecom dataset. models for the various QA categories. Across the chunking strate-
gies, higher performance is most commonly seen when using the en-
tire graph JSON as one chunk, for both models. Better performance
The fine-tuned model was evaluated on the test set for predicting is obtained for node-related questions, followed by decision-related
the image categories. The performance of the model on various and finally, edge-related questions.
categories of images is shown in Fig. 6. We observe that the ac- .
curacy of prediction is high for images categories like icons and 5 Conclusions and Future Work
equations, while it is above 80% for sequence diagrams, screenshots In this work, we have considered an approach to introduce flow-
and flowcharts. chart images with dominant textual content into retrieval (for RAG)
We also show the confusion matrix for the test set indicating the using textual embeddings. We first categorize images present in the
correct and incorrect classification of images in the test set in Fig. 7. domain dataset using a fine-tuned DIT model. We observe that the
We observe that most of the flowchart images are classified correctly, accuracy of flowchart category of images is sufficiently high. Next,
while some are misclassified as block diagram, schematic diagram or the flowchart images are converted to graph structures using a fine-
others. This is expected since these images can be similar and belong tuned VLM. Here, we show that the fine-tuned VLM has lower GED
to multiple categories. However, since most of the flowchart images for the flowchart graph representations. These graph structures are
are categorized correctly, we use this classifier in the pipeline. The then embedded into text-based vector store and benchmarked for
performance can potentially be improved as more annotated images retrieval accuracy on a QA dataset based on these graphs. We ob-
are available for fine-tuning. serve that embedding the whole graph as one vector shows higher
accuracy. This is because the chunk of data embedded includes all
4.2 VLM for graph representation the information related to the nodes and edges of the graph. This
The results for fine-tuning of the Qwen2-VL [17] VLM are shown is also shown to have better performance when the QA category is
in Table 3, for various parameter values. The columns indicate node-related.
the average number of nodes and edges in the ground truth graph Future work includes evaluation of generator output when JSON
JSON representations, the number of nodes after the transformation structures are passed as context to textual generator component
operations and the number of edges detected for the model outputs. of RAG. Analyzing cases involving errors in graph generation and
The GED metric is shown in the last column. We obtain the lowest retrieval, and performance with interspersed (document) text would
GED of 2.74 using both Lora R and @ as 512 for the finetuned Qwen2- be of interest. Additionally, extending the capabilities to other types
VL model, which is a significant improvement over the base model
which has a GED of 10.21 on the test set. ? TeleRoberta model is much smaller than bge-large model in terms of parameter size.


--- Page 7 ---

Graph Representations for Flowchart QA KDD °’25 Workshop on Structured Knowledge for LLMs, August 04, 2025, Toronto, CA
Class-wise Accuracy on Test Set
Sequence Diagram
Screenshot
Schematic Diagram
Others
Icon
Hardware
Graph
Flowchart
Equation
Block Diagram Po
i) 20 40 60 80 100
Accuracy (%)
Figure 6: Performance of the classifier on the test set for various image categories from PI docs.
Model Avg. #Nodes Avg. #Edges Avg. #Nodes | Avg. #Edges | Avg. Graph
(Ground Truth) | (Ground Truth) | Detected Detected Edit Distance (GED)
Flowcharts from test set of Flowlearn dataset
Qwen2-VL Base 6.36 6.82 6.55 8.57 10.21
Qwen2-VL FT - Lora R=8, Alpha=16 6.36 6.82 6.3 6.23 4.24
Qwen2-VL FT - Lora R=32, Alpha=32 6.36 6.82 6.2 6.53 5.09
Qwen2-VL FT - Lora R=128, Alpha=128 | 6.36 6.82 6.34 6.43 4.43
Qwen2-VL FT - Lora R=256, Alpha=256 | 6.36 6.82 6.35 6.7 3.82
Qwen2-VL FT - Lora R=512, Alpha=512 | 6.36 6.82 6.36 6.42 2.74
Flowcharts from PI Docs
Qwen2-VL FT - Lora R=512, Alpha=512 | 12.54 11.77 12.32 11.11
Table 3: Graph Metrics for fine-tuned VLM with various parameter settings reported for test set of Flowlearn dataset and
flowcharts from PI Docs.
Embedding Model bge-large TeleRoBERTa
Chunking Approach
Embeddings of graph structures only for retrieval
Each node as one chunk 56.17% | 65.33% | 66.93% | 57.17% | 62.74% | 65.93%
All the nodes as one chunk 50.29% | 68.12% | 75.23% | 49.80% | 71.91% | 76.89%
Entire graph JSON as one chunk | 53.19% | 71.12% | 78.29% | 49.06% | 71.71% | 76.69%
Embeddings of graph structures interspersed with text for retrieval
Each node as one chunk 41.05 55.53 59.76 32.42 44.08 48.86
All the nodes as one chunk 38.84 43.63 43.82 30.92 38.33 42.29
Entire graph JSON as one chunk | 30.08 31.47 32.03 24.27 28.15 39.83
Table 4: Retriever performance of chunking approaches for the embedding models, best top-k values are indicated in bold
for (i) only embeddings of graph structures considered for retrieval, and (ii) with embeddings of graph structures and text
considered for retrieval.
of diagrams like UML Sequence diagrams which have a semantic References
structure are areas of potential future research for the community. [1] 3GPP. 2022. 3GPP Release 18. Technical Report. Accessed: 2024-05-19.
[2] Zeina Abu-Aisheh, Romain Raveaux, Jean-Yves Ramel, and Patrick Martineau.
2015. An exact graph edit distance algorithm for solving pattern recognition
problems. In 4th International Conference on Pattern Recognition Applications and


--- Page 8 ---

KDD ’25 Workshop on Structured Knowledge for LLMs, August 04, 2025, Toronto, CA Soman et al.
Chunking Approach
bge-large
Each node as one chunk 46.11 | 59.20 | 40.67 | 63.72 | 77.53 | 60.03 | 66.22 | 81.43 | 63.61
All the nodes as one chunk 46.96 | 52.44 | 47.88 | 65.12 | 68.27 | 65.61 | 70.33 | 78.19 | 66.42
Entire graph JSON as one chunk | 47.35 | 52.57 | 51.98 | 67.97 | 70.43 | 70.15 | 71.49 | 77.41 | 77.45
TeleRoBERTa
Each node as one chunk 43.21 | 56.71 | 37.02 | 58.12 | 73.72 | 55.73 | 63.32 | 78.55 | 59.61
All the nodes as one chunk 47.13 | 50.29 | 49.77 | 61.36 | 65.22 | 64.44 | 67.80 | 73.41 | 65.16
Entire graph JSON as one chunk | 46.75 | 49.20 | 51.87 | 65.64 | 66.09 | 69.08 | 71.22 | 72.34 | 70.77
Table 5: D, N, E indicative of Decision based, Node based and Edge based QA. Retriever performance of chunking approaches
for the two embedding models. Best top-k values are indicated in bold for various chunking approaches.
ACM SIGIR conference on Research and development in information retrieval (2006).
. . [8] Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang,
Confusion Matrix on Test Set and Ying Shan. 2024. Seed-bench: Benchmarking multimodal large language
Block Diagram] 23 ° 2 ° 0 ° 4 7 0 3 models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition. 13299-13308.
« 5 ° 5 5 5 ° 5 ° } [9] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and
auation Yong Jae Lee. 2024. LLaVA-NeXT: Improved reasoning, OCR, and world knowl-
edge. https://llava-vl.github.io/blog/2024-01-30-llava-next/
Flowchart 7 2 0 2 @ 0 0 0 0 0 0 [10] Huitong Pan, Qi Zhang, Cornelia Caragea, Eduard Dragut, and Longin Jan Late-
cki. 2024. Flowlearn: Evaluating large vision-language models on flowchart
Graph 0 0 0 5 0 0 4 0 0 0 understanding. In ECAI 2024. IOS Press, 73-80.
[11] Sujoy Roychowdhury, Nishkarsh Jain, and Sumit Soman. 2024. Unlocking tele-
com domain knowledge using Ilms. In 2024 16th International Conference on
ov Hardware 0 oO 0 0 1 0 oO 2 0 oO . .
3 COMmunication Systems & NETworkS (COMSNETS). IEEE, 267-269.
g [12] Sujoy Roychowdhury, Sumit Soman, HG Ranjani, Neeraj Gunda, Vansh Chhabra,
© kon} 0 ° ° ° ° 3 ° ° ° ° and Sai Krishna Bala. [n. d.]. Evaluation of RAG Metrics for Question Answering
in the Telecom Domain. In ICML 2024 Workshop on Foundation Models in the
Others 3 0 3 3 0 (0) 23 8 3 (0) Wild.
[13] T. Saraiva, M. Sousa, P. Vieira, and A. Rodrigues. 2025. Telco-DPR: A Hybrid
Schematic Diagram] 2 0 2 0 0 0 6 19 0 0 Dataset for Evaluating Retrieval Models of 3GPP Technical Specifications. In
2025 IEEE Wireless Communications and Networking Conference (WCNC). 01-06.
0 1 ° ° 0 ° 1 1 99 ° doi:10.1109/WCNC61545.2025.10978393
Screenshot [14] Shubhankar Singh, Purvi Chaurasia, Yerram Varun, Pranshu Pandya, Vatsal
Gupta, Vivek Gupta, and Dan Roth. 2024. Flowvqa: Mapping multimodal logic
Sequence Diagram) 0 ° ° ° ° ° 1 ° ° ° in visual question answering with flowcharts. arXiv preprint arXiv:2406.19237
(2024).
ss ee & EF re s < s [15] Sumit Soman and Ranjani HG. 2023. Observations on LLMs for telecom domain:
ro sf RS So § capabilities and limitations. In Proceedings of the Third International Conference
S * on AI-ML Systems. 1-5.
s om [16] Sumit Soman and Sujoy Roychowdhury. [n. d.]. Observations on Building RAG
Predicted label Systems for Technical Documents. In The Second Tiny Papers Track at ICLR 2024.
[17] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin
: . . . : Chen, Xuejing Liu, Jialin W: Wenbin Ge, et al. 2024. 2-vl: Enhanci
Figure 7: Confusion matrix on the test set for categorization hen, Xuejing Liu, Jialin Wang, Wenbin Ge, et a Qwen?2-vi: Enhancing
> vision-language model’s perception of the world at any resolution. arXiv preprint
of images from the PI dataset. arXiv:2409.12191 (2024).
[18] Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighoff. 2023.
C-Pack: Packaged Resources To Advance General Chinese Embedding.
arXiv:2309.07597 [cs.CL]
Methods 2015.
[3] Haoyu Han, Yu Wang, Harry Shomer, Kai Guo, Jiayuan Ding, Yongjia Lei, Ma-
hantesh Halappanavar, Ryan A Rossi, Subhabrata Mukherjee, Xianfeng Tang, et al. A Image Cate gories in Telecom Dataset
2024. Retrieval-augmented generation with graphs (graphrag). arXiv preprint ; ; ; ;
arXiv:2501.00309 (2024). Representative examples of various types of images present in the
[4] Henrik Holm. 2021. Bidirectional encoder representations from transformers telecom (PI) dataset are shown in Fig. 8.
(bert) for question answering in the telecom domain.: Adapting a bert-like lan-
guage model to the telecom domain using the electra pre-training approach. . . .
[5] Athanasios Karapantelakis, Mukesh Thakur, Alexandros Nikou, Farnaz Moradi, B Ambiguous Flowchart images in 3GPP
Christian Olrog, Fitsum Gaim, Henrik Holm, Doumitrou Daniil Nimara, and Vin-
cent Huang. 2024. Using large language models to understand telecom standards. documents
In 2024 IEEE International Conference on Machine Learning for Communication A representative image from the 3GPP document that is ambiguous
and Networking (ICMLCN). IEEE, 440-446. fl h is sh in Fig. 9
[6] Aryan Keskar, Srinivasa Perisetla, and Ross Greer. 2025. Evaluating multimodal as a flowchart is shown in Fig. 9.
vision-language model prompting strategies for visual question answering in ;
road scene understanding. In Proceedings of the Winter Conference on Applications Received 30 May 2025
of Computer Vision. 1027-1036.
[7] David D. Lewis, Gady Agam, Shlomo Engelson Argamon, Ophir Frieder, David A.
Grossman, and Jefferson Heard. 2006. Building a test collection for complex
document information processing. Proceedings of the 29th annual international


--- Page 9 ---

Graph Representations for Flowchart QA KDD ’25 Workshop on Structured Knowledge for LLMs, August 04, 2025, Toronto, CA
..
<>
aoe
[=> <> C2)
(a) Block Diagram (b) Flowchart
° XBVU KLMN
cox
os A Message 1
on bY Message 2
KLMN
" 2 By Pa ps By ©) Message 3
ABC
(c) Graphs (d) Hardware Diagrams (e) Icons
VDB713
x
lave) | SMPRIA7OR No interfering objects
LET in front of the TBS
(f) Instruction Diagrams (g) Schematic Diagram (h) Representation Diagrams
DEF | Configuration Generator
Configuration Generator
Basic datz ABCDET23 : No Configuration Type selected qwertz
Value XYZ98765432 hd
Setting 4567 PQLM RSTUVI2 binms
Ges --- sernicolon-sepera- ¥
Configuration Type
FG Option
-ABCD
H Radio A “ILMS
| Radio B
pqrst
Figure 2 Basic Template Generator Menu Lone
The menu of the Basic Template Generator incluodes following UI elements:
- Action buttons rtyuk
- Settings
(i) Screenshots (j) Sequence Diagrams
Figure 8: Representative images for various categories from the proprietary telecom dataset. (Note: content has been obfuscated
based on the confidentiality of data involved.)


--- Page 10 ---

KDD ’25 Workshop on Structured Knowledge for LLMs, August 04, 2025, Toronto, CA Soman et al.
Hat :
, | | x Splitter
il
Fader 1
r Rx 3
i x Splitter
Fader 4 BRx UE
| AWGN | under tests
i
7 Z Fader 2 Z AWGN
T: Splitter = Splitter
Facer ss
Fall [Awan]
AWGN
x Splitter
Figure 9: A sample of ambiguous flowchart image in 3GPP document. One can consider this to also be a block diagram.
