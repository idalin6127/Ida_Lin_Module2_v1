

--- Page 1 ---

Cascaded Information Disclosure for Generalized Evaluation of Problem
Solving Capabilities
Yunxiang Yan Tomohiro Sawada _ Kartik Goyal
College of Computing
Georgia Institute of Technology
{ryan.yunxiang.yan, tsawada, kartikgo}@gatech. edu
Abstract ability to pick the correct choice among distrac-
While question-answering (QA) benchmark tors or to produce a correct numerical answer to a
Vay performance is an automatic and scalable mathematical problem doesn’t measure the qual-
N method to compare LLMs, it is an indirect ity of strategies these models use to arrive at that
QI method of evaluating their underlying problem- answer. There is a pressing need (Alzahrani et al.,
— solving capabilities. Therefore, we propose a 2024; Gan et al., 2024; Li et al., 2024b) for direct
= holistic and generalizable framework based on evaluation of problem-solving capabilities as they
—_ cascaded ne discioaur © thy Prowiess a are better aligned to the actual use-cases of LLMs.
xa) more accurate estimare or ihe moce'’s_ prooiem- For example, while the GPQA dataset (Rein et al.,
solving capabilities while maintaining the scal- : : - .
— ability and automation. This approach collects 2023) contains multiple-choice questions (MCQs)
_) model responses in a stagewise manner with that indirectly test the graduate-level knowledge of
UO each stage revealing partial information about the models, the corresponding real-world use-case
wh the question designed to elicit generalized rea- is more subjective — the model should possess the
S) soning in LLMs. We find that our approach ability to have an accurate open-ended conversa-
; ;
not only provides a better comparison between tion with the user about advanced concepts. This
— LLMs, but also induces better intermediate ability is better reflected in the strategies that the
> traces in models compared to the standard QA .
: -. . . LLM uses to answer GPQA questions. Hence, the
\O paradigm. We empirically verify this behav- ; ; ~ ;
t™ ior on diverse reasoning and knowledge-heavy focus of this paper is to directly evaluate the inter-
t~ QA datasets by comparing LLMs of varying nal problem-solving capabilities of the models in a
ag sizes and families. Our approach narrows the holistic, yet scalable and generalizable manner.
~ performance gap observed in the standard QA We propose an evaluation framework called cas-
>) evaluation settings, indicating that the prevalent caded information disclosure which focuses on elic-
Via) indirect QA paradigm of evaluation overesti- iting and estimating the problem-solving capabili-
N mates the differences in performance between . . . .
oe : . ties reflected in the intermediate traces of the mod-
> models. We further validate our findings by : : :
ne extensive ablation studies. els. It involves modification of standard QA tasks
< as follows. We first transform the question into its
bat : more generalized form and then design a scheme
a} 1 Introduction ae ; ; ene
to partition this form into non-overlapping parts.
While general-purpose LLMs have become ubiqui- Then we disclose these parts to the model under
tous today, evaluating them holistically remains — evaluation in a stagewise manner. While this frame-
a massive challenge. Primarily, these models | work accommodates multiple stages/parts, the final
are compared against each other by their per- _ stage in all instantiations is a verifiable projection
formance on a handful of benchmark tasks and stage that projects the model’s responses in the
datasets (Liang et al., 2023; Srivastava et al., earlier stages to an automatically evaluatable/veri-
2023; Wang et al., 2019; Sarlin et al., 2020) dom- _fiable form. This progressive question disclosure
inated by various kinds of question-answering leads to a more accurate assessment of the sub-
tasks (Hendrycks et al., 2021; Lin et al., 2022; jective problem-solving capabilities of the models
Rajpurkar et al., 2016; Rein et al., 2023). While — while the verifiable projection stage maintains the
objective and scalable, to estimate the underlying scalability and automation of evaluation. We instan-
problem-solving capabilities of models, this ap- __ tiate this framework for two distinct QA modes: i)
proach is indirect — merely judging a model onits _—_ multiple-choice QA, and ii) reasoning-based math
1


--- Page 2 ---

Q Stem: A small iron block is
dropped from a height of 30m.
B) (> 19m/s) and (< 21m/s) <Ideation EOS The velocity might be approximately \sqrt(4*\pi)m/s but I need to know the
C) (> 49 km/hr) and (< 51 km/ mass and options to answer. <\Ideation Answer>
hr) Subjective eval: LLM-judge
D) < 10m/s
Options:... .
Qstem..
arr _ <Ideation Answer>... <\Ideation
Options:.... = = 5 Answer>
on satisties stage-1 answer. <\ Projector because you need the knowledge of the options
; ....through elimination of think> to answer.<\Judge think>
<Projecte d Answer> A<\Projected <Judge Answer>Correct<\Judge Answer>
Figure 1: Demonstration of MCQA instantiation of our Cascaded Information Disclosure evaluation framework
against Standard evaluation and LLM-based Subjective evaluation of problem-solving capabilities. The standard
evaluation paradigm picks the correct option but has poor justification for the answer. Cascaded Information
Disclosure strategically generalizes the question and exposes parts of it to the model in a stagewise manner. The
free-form response generated during ideation phase reflects model’s poor problem-solving ability. LLM-based
subjective evaluation is an appealing alternative to judge the ideated response but fails in the example shown —
it supports the trivial conclusion in the incorrect ideation. Verifiable Projection while objective and automatic,
correctly penalizes the incorrect ideated response by picking the wrong option.
word problems. is that an answerer (human or machine) will em-
Comparing a wide variety of models of different ploy more detailed reasoning and knowledge-based
sizes and families (§ 3.1), our approach narrows the _ strategies and reveal more information in their an-
performance gap observed in the standard evalua- _ swers if the question is broken down into a series
tion settings, indicating that the prevalent indirect of leading questions. Therefore, our approach in-
evaluation paradigm overestimates the differences volves exposing partial information about the ques-
in performance between models. Overall, we find _ tion to the answerer model in a cascade of stages
that our approach not only provides a better esti- and eliciting potentially detailed and meaningful re-
mate of LLMs’ underlying reasoning capabilities, | sponses from it at each stage. More concretely, the
but also elicits better performance and intermediate input question Q is first converted into an abstract
traces from the models compared to the standard form Q. The abstract question is then decomposed
QA paradigm — an effect primarily driven by the de- _ into n non-overlapping subsets Q = Uj, Qi. Our
composition of the questions into separate foci. We approach then solicits responses FR; from the model
further validate the soundness of our observations — by presenting these subsets in a predetermined or-
by extensive ablation studies. der and conditioning each response on the corre-
To summarize, we i) propose a novel generalized  Sponding subset and the response in the previous
framework to test the problem-solving capabilities Stage: Ry <— Qi, Ri-1. The response of the final
of LLMs that supports automation and extensibility, | Stage is considered to be the model’s answer to
ii) empirically study our framework via concrete __ the full question: A(Q) = Rn | Qn, Rn-1. Cru-
automatic instantiations across multiple QA tasks _C1ally, the response at any stage doesn’t explicitly
and LLMs, and iii) create manual instantiations of | depend on the subparts of the question at the ear-
this framework by annotating two datasets that will lier stages — the only information the model can
be publicly available. gather about the previous stages is through the re-
sponse generated one stage prior. This promotes
2 Cascaded Information Disclosure detailed and informative responses from the model
at each stage which, as we show in our results,
The central guiding conjecture (Wood et al., 1976; is the primary driver for eliciting better problem-
Collins and Stevens, 1983) behind our approach _ solving strategies from the models. An appropriate
2


--- Page 3 ---

decomposition procedure underpins the success of response and the variable assignment as input. The
our approach and needs to be custom-designed for | mathematical formula is turned into executable
every instantiation of this abstract framework. code that accepts the variable assignments as ar-

In our experiments, we instantiate this frame- guments to yield a numeric answer to the origi-
work for two different types of questions: multiple- nal question. This instantiation requires consider-
choice questions, and math word problems. While able manual intervention. Therefore, we use the
the instantiations are procedurally different, there © GSM8K (Cobbe et al., 2021) dataset for original
are some commonalities between the two instantia- | questions whose generalized form is constructed us-
tions. Both of them decompose the question into —_ ing templates in prior work (Mirzadeh et al., 2024).
two stages (n=2) that we call generalized ideation
and verifiable projection stages. The generalized 3 Experimental Setup
ideation stage presents a partial generalized view We empirically compare the effectiveness of the
of the original question to the model such that the . . . .

cascaded information disclosure evaluation frame-

model generates a broad and general response Rj. .

; _ . work against other prevalent frameworks by eval-
The verifiable projection stage takes this response . P :

’ ; ; uating a variety of open-weight models span-
along with the rest of the information in the orig- . . . > d model famili
inal question (question residue) to project R, to ning various sizes (7B to 32B) and model families
ma he vel q bl , J The (Llama (Grattafiori et al., 2024), Qwen (Qwen et al.,
an objectively measurable answer format. There- 9995) Gemma (Team et al., 2024), and Phi (Abdin
fore, the generalized ideation stage produces the . . .

a . : et al., 2024)). Specifically, we use the instruction-
subjective response detailing the problem-solving .
: - finetuned checkpoints for the models that are capa-
strategies employed by the models, and the veri- . . .
oo ; ble of following instruction prompts in a zero-shot
fiable projection stage ensures that the evaluation . os .
: . ; QA setting. (Details in Appendix)
remains automatic and scalable by using standard . . . .
. . . Datasets: As described in§ 2, we instantiate our
evaluation metrics for the projected response. . . .
cascaded information disclosure framework for
2.1 MCQA Instantiation problem-solving evaluation for two types of ques-
. ; . tions: MCQs and math word problems. For the
We break the MCQ into "wo parts: the question MCQA format, we picked a) the test split of ARC-
stem, and the options containing the answer. There- .
. . . Challenge (Clark et al., 2018) dataset which con-
fore, the generalized ideation stage only gets to . . .
look at the general question without the knowledee sists of grade school level science questions for

& a 8° MCQA format and, b) the GPQA dataset which

of the answer candidate and induces the model : . .
; . consists of challenging graduate-level science ques-
to produce a detailed constructed response (Liv- . . . .
. or tions written by domain experts. Specifically,
ingston, 2009) to answer the question in a free- . .
aor ; ; we use the high-quality subset of the full GPQA
form subjective manner. This response is then . . .
. . . dataset: GPQA-Main (448 questions), and its more
passed, along with only the candidate options, . .
; or ; challenging subset GPQA-Diamond (198 ques-
to the verifiable projection stage. This stage as- . . .
: ; tions). It is reported in the GPQA paper (Rein et al.,
sesses the model’s free-form response and picks . . .
; . 2023) that highly skilled non-expert human valida-
the option that most appropriately matches the .

We conduct experiments on different tors only reach 30.4% and 22.1% accuracies on
response . . GPQA-Main and GPQA-Diamond despite spend-
MCQA datasets designed to test for reasoning and, . . .

ing on average over 30 minutes with unrestricted
advanced graduate-level knowledge. .
access to the internet. For math word problems,
2.2 Math Word Problem Instantiation we use GSMBK to obtain original questions. Previ-
a . . ous work Mirzadeh et al. (2024) created 100 sym-
The original math word problems are rewritten into . . . ot
. . bolic templates from 100 questions in the original
a templated form that is more general and specific . .
Ae . . GSM8K dataset by manually annotating the enti-
entities are replaced by abstract variables. This . . .
. . ; ties (numeric values and categorical values) that
generalized question is presented to the model in . .
; . ; ; can be abstracted into variables. We extend these
ideation stage in which the only correct way to . . .
Lo, templates by creating consistent assignments to the
answer the question is to reason about the prob- . . Le oe
; variables in the templates, resulting in a derivative
lem and unknown variables, and produce a general 1 ae
. . ; dataset we call GSM-General’ consisting of 100
mathematical formula for the generalized question.
The verifiable projection stage takes the ideation 'We will make this converted symbolic version of GSM8K
3


--- Page 4 ---

template-variable assignment pairs that map to the the 100 original GSM8K questions.
100 original questions in GSM8K dataset. . .
Inference Details: For prompt engineering, we 3:1-2 Cascaded Information Disclosure
adopt zero-shot, chain-of-thought prompting (Wei | As described above, we instantiate both the ques-
et al., 2023) to mimic the most common real-world _ tion types with two stages: generalized ideation and
LLM use case. Prompt templates are available in _ verifiable projection. In the generalized ideation
appendix 7.2. We specifically crafted the prompts Stage, we break the original question (or its trans-
for all the settings including the baseline methods _—_ formed abstract form) into a generalized question
so as to achieve maximal performance on the tasks and question residue. In the MCQA setting, we
under each method. Specifically, we found that construct the generalized question by concealing
describing our cascaded setup for question disclo- the options, which will then be used as question
sure to the models via the prompt resulted in stable _ residue in the subsequent stage. As discussed in
behavior under our approach. We use XML tags _ § 4.5, we also experiment with obtaining the gener-
based answer format in both prompting and answer _ alized question by concealing the options and man-
extraction. We use greedy decoding whichis the de- _ ually rewriting the question stem to make it option-
fault setting for all three selected datasets. Foreach independent and general. In math word problem
model, we set the context length to the maximum __ setting, we construct the generalized question by
supported by the model and we set a uniform cap _ converting the original question into its symbolic
of 8192 for maximum number of generated tokens _ form that replaces entities and quantities in the orig-
to facilitate a fair comparison across models. inal question with abstract variables, leaving the
value assignment to the variables involved as the
3.1 Evaluation Strategies Compared question residue. In the prompt for generalized
3.1.1 Standard Evaluation ideation stage, we explicitly specify that reiteration
This evaluation setting is the most widely used of the question is prohibited to restrict its leakage
evaluation method for the datasets we work with. 1° the next stage. o. ;
We further enhanced the standard evaluation by In the verifiable projection stage, we Project the
developing custom prompts and answer extrac- generalized reasoning traces in the ideation stage
tion methods to ensure maximal performance on to either an option in MCQA or a numeric result
the datasets that would serve as difficult baselines math word problem through a projector. The
for our approach to beat. For MCQA type, the projector for MCQA tasks is an LLM whose task
models take in the full original question: stem !S to pick the most relevant option based solely on
(What is Newton’s Third Law of Motion evaluation subject’s ideation stage traces (no ac-
about?) and candidate options (A: Inertia, B: cess to generalized question). We also explicitly
Force and acceleration, C: Momentum, D: instruct the projector LLM to abstain from solving
Action-Reaction) as inputs, and provide a re- the original problem and only focus on matching to
sponse from which the answer index (A, B, C, the correct option to ensure that the accuracy on an-
or D) is extracted.” swers predicted by the projector provides a reliable
A math word problem is a mathematical exer- estimate of the quality of the open-ended Teason-
cise presented as a scenario, requiring the applica- 18 trace generated in the ideation stage. Similarly,
tion of mathematical reasoning and computation the P rojector for the math word problem 1S only
to solve a real-world or hypothetical situation and provided the trace from the ideation stage and 1S
provide a numerical answer. For evaluation, the tasked with producing an executable Pythonic eX-
prompt contains requirements on the format of the  PFSS107 of the logic and formula expressed in the
final answer. We use Im-evaluation-harness’s im.  1¢@ation stage. This code is then executed with
plementation to evaluate models’ performance on the variable values to verify the correctness of the
answer produced. We experiment with different
dataset, GSM-General publicly available. kinds of projectors: a) the LLM being evaluated
We use two implementations of standard MCQA base- :
lines: LM Evaluation Harness ((Gao et al., 2024); denoted as the projector (self), b) an external Pp erformant
“LMH”) and a custom implementation. We found that LMH’s | Open-weight projector (model Phi), c) a blackbox
simple prompts and answer extractions led to high parsing frontier projector model GPT-40 (Open Al et al.,
failure rate. To provide a fairer comparison of model perfor- 2024), and d) a rule-based projector that doesn’t
mance, we develop an advanced MCQA baseline that greatly >
improves model’s instruction following on formatting. use any LLMs. For rule-based substitute of LLM
4


--- Page 5 ---

projector in MCQA, we implementa sentence-level | For each evaluation method, we measure perfor-
BLEU score metric (Post, 2018) for matching the = mance gap as the difference between the highest
first stage traces to the ground-truth answer. Inthe and the lowest scoring model. We observe that
math word problem setting, we implement afunc- —_ our approach results in a significantly narrower gap
tion that extracts the expression stated in answer between the models compared to the standard eval-
tags from the ideation stage output. If executable, uation. For example, the gap under standard eval-
this expression is used to produce the answer by _ uation for GPQA-Diamond (37.4) is much higher
filling variable values from the question residue. than the gap (12.1) when GPT-4o is used as the
verifying projector. Moreover, the more powerful
3.1.3 Subjective evaluation of Generalized the verifying projector, the lower the performance
Ideation Traces gap. This suggests that the standard evaluation
While verifiable projection stage makes our ap- scheme prevalent today overestimates the differ-
proach automatic and reliable, itis not the only pos- — ence between language model’s capabilities.
sible automatic approach to evaluate the problem-
solving capabilities of the model elicited in the gen- 4-2 Cascaded Information Disclosure
eralized ideation stage. We also compare our setup Estimates Problem-solving Capability
against strategies for automatic subjective evalua- | Generalized questions make QA tasks harder:
tion of the first stage traces. We experiment with The generalized question used in our framework is
LLM-as-Judge projector (Zheng et al., 2023) for more difficult than the original questions, by their
both MCQA and math word problem settings. The nature. For MCQA, the answer choices are omit-
LLM judge, armed with the ground truth answer, ted in the generalized questions, thereby providing
is tasked to project the ideation trace to a binary __ the evaluation subjects with strictly less informa-
verdict "Correct" and "Incorrect". tion compared to standard evaluation methods. For
GSM-General, the question is transformed into an
3.2 Evaluation Metrics abstract symbolic question, forcing the models to
Following the description above, we use the term _utilize their reasoning capabilities.
accuracy to refer to two kinds of metrics whose The increase in difficulty is naturally reflected
usage is differentiated by context: a) objective ac- _ by decrease in performance of larger models (14B+
curacy, which is a measure of match between the — parameters) on MCQA tasks in Table | under our
ground truth answer and the projected answer, and evaluation par adigm. A qualitative example demon-
b) subjective accuracy, which is the rate of correct- _Strating this is provided as Phi-4’s raw reasoning
ness as judged by an LLM. Thus, the standard traces under standard MCQA 2 and cascaded in-
baselines and systems employing the verifiable | formation disclosure 3 for question #996 in ARC-
projector use objective scoring, while LLM-as-a- Challenge dataset. Under standard MCQA setting
judge methods use subjective scoring that decides where options are provided, Phi-4 only analyzes the
whether the ideation trace is deemed correct or _ four options to get the correct answer whereas in
not. Subjective accuracy is heavily biased towards _ our evaluation setting, Phi-4 falsely identified "nu-
judge’s style and hence is not directly comparable cleus" as not being part of the atom during ideation
to objective accuracy. We also report the parsing and its ideation trace is projected to the Incorrect
failure rate (in Appendix) of answer extraction. The — answer by our verifiable projector correctly. The in-
failure is caused by the answering model’s inability _ crease in difficulty posed by our framework forces
to follow answer formatting instructions. models to engage with the task at a deeper level,
thereby contributing to superior evaluation of the
4 Empirical Analysis model’s problem-solving capabilities.
. . Generalized questions elicit better traces from
4.1 Cascaded Information Disclosure the models: By contrast, we observe the oppo-
Narrows the Gap between Models site trend for the smaller models (<10B) and the
In Table 1, we present accuracies of LLMs eval- GSM8K dataset: the model performance is signifi-
uated on four different benchmark datasets using cantly better under our framework with verifiable
various evaluation paradigms. We report three dif- projection than the standard evaluation setting. In
ferent LLM projectors for the verifiable projection _ fact, stronger projectors yields better performance.
stage: Phi-4, GPT-40, and ideation model itself. This is caused by the improvement in the reasoning
5


--- Page 6 ---

Table 1: Comparison of Cascaded Information Disclosure based evaluation with verifiable projector with standard
(Std.) evaluation. Experiments with three different kinds of projectors are reported: the LM under evaluation (self),
external open-weight LM (Phi-4), and blackbox frontier model (GPT-40). Performance Gap refers to the difference
between the best performing model and the worst performing model.
Model} | [Projector | [Projector | [Projector |__| Projector |
Std. Self Phi-4| Std. Self Phi-4 GPT-40]| Std. Self Phi-4 GPT-40/Std. Self Phi-4
Llama 3.1-8B |82.5 80.4 87.7 | 20.7 31.8 32.8 35.3. |23.7 28.3 33.0 30.8 | 32 40 42
Gemma 2-9B |90.4 84.6 90.1 |30.30 31.3 31.8 35.3. |32.4 30.6 32.6 348 | 21 70 74
Qwen 2.5-7B | 87.2 82.8 89.0 )32.32 30.3 34.8 37.4 |31.0 31.9 30.1 29.2 | 76 75 70
Qwen 2.5-14B |92.1 88.8 90.9 | 41.4 40.4 41.9 37.9 |38.2 36.2 37.7 37.3 | 80 85 84
Phi-4 (14B) [95.7 90.8 90.8 | 58.1 51.5 51.5 47.5 [52.9 45.3 45.3 47.1 | 52 91 91
Qwen 2.5-32B |93.3 91.0 92.9 | 47.0 42.4 44.9 465 |43.1 41.5 40.8 42.6 | 87 95 95
Perf. Gap A /13.2 10.7 5.2 | 37.4 21.2 19.7) 12.1 |29.2 16.9 15.2 17.9 | 66 55 53
traces generated by the models under our evaluation —_ evaluation harness), LMH+ (LM evaluation harness
framework, as we observe from manual inspection. augmented with additional formatting and parsing
In one of the GSM8K outputs (Listing 1), we ob- instructions), and custom prompting that we de-
serve that the Gemma-9B’s outputs are incoherent signed specifically for the standard MCQA setup
past the third sentence for the standard evaluation 3.1. We observed that LMH and LMH+ evalua-
method. In contrast, the reasoning traces for the tion setups are completely unsuitable for the Phi-4
projector method are semantically coherent, laying model (parsing failure rate of 90%+ on GPQA).
out each step coherently and explicitly. We observe ‘This is because Im-evaluation-harness evaluation
similar behavior for GPQA outputs. In the standard = system doesn’t apply any chat template even for
evaluation method (Listing 4), Qwen-7B’s reason- _instruction-finetuned models and Phi-4 tends to
ing traces for a quantum mechanics problem are _ produce empty output when the input is not aligned
correct up to the middle, after which it hallucinates with its prompt template. In general, we observe
its expression to match one of the provided answer __ that the standard widely used LMH system suffers
choices. For the projection method (Listing 5), the | from high answer parsing error rates for a major-
model generates a correct reasoning trace from start ity of our models: upwards of 40% and 30% for
to finish, without any hallucinations. Gemma2-9B and the Llama3.1-8B models, and
Our framework elicits higher quality traces from around 2-5% for the Qwen models. LMH+ reduces
the model, thereby providing a more faithful esti- _ these failure rates but they remain significantly high
mation of its problem-solving capability. for the Llama and Gemma models (15-20%). This
indicates that models struggle to adhere to the im-
4.3 Separation of Concerns Improves plicit formatting assumptions of the Im-evaluation-
Instruction Following Quality harness framework. Our custom standard method
Recent work on RLVF has shown that improve- which applies correct chat template for each model
ments in MCQA are often driven not by enhanced __ via careful prompt engineering and answer extrac-
reasoning, but by better output formatting capa- tion strategies detailed in 3.1 further reduces pars-
bilities introduced via fine-tuning or reinforcement __ ing failures to below 10% for most models except
learning (Shao et al., 2025). This suggests that stan- the Llama model, possibly because of length issues.
dard evaluations underestimate models’ problem Contrastingly, our evaluation setting achieves
solving abilities due to brittle evaluation pipelines zero parsing failures across all models. This is
that rely on rigid regex-based answer extraction. expected — decomposition of the answering mech-
To quantify the effect of formatting issues, we com- anism into ideation and projection stages enables
pare parsing failure rates (full table in Appendix — disentangled focus on each stage leading to greater
Table 9) between our two-stage method with Phi-4 | format adherence. More importantly, the reduction
as the final projector and the standard evaluation _(or elimination) of formatting issues with custom-
paradigm. For standard evaluation paradigm, we __ standard and our approach indicates that the perfor-
compare three systems: LMH (widely used LM mance trends in Table 1 are an accurate reflection
6


--- Page 7 ---

the models’ performance on QA tasks, having ac- closure framework is that some multiple-choice
counted for a commonly observed confounding questions may not be fully self-contained once
factor related to formatting. the candidate options are removed. Therefore, for
challenging GPQA-Diamond questions, we pro-
4.4 Oracle for Generalized Ideation jected the original MCQ (Q) to a modified abstract
We evaluate the quality of two projection strategies | form (Q) by manually annotating the question
in our framework: a) LLM-judge projector, b) veri- | stems such that they are self-contained. Table 3
fiable projector. Specifically for GPQA, we bypass shows the difference between performance under
the LLM-based ideation stage and instead use hu- _ our evaluation approach with Phi-4 as the projector
man expert-written explanations as inputs to the —_ on the annotated dataset and the unmodified dataset.
projection stage. These explanations are provided § We only notice small differences in either direction
by the expert annotators and are included as part of | across LLMs. These results indicate that manual
the GPQA dataset. Thus, in this setting the LLM- annotation is not necessary for effective cascaded
judge projector should score 100% because all the information disclosure evaluation. This finding re-
explanations are correct by design. Also, ahigh- _inforces the practical scalability of our approach,
quality verifiable-projector should always pick the allowing researchers to repurpose existing MCQA
correct option congruent with the ground-truth ex- datasets for improved evaluation without incurring
planation. The judge setting in Table 2 assumes ac- _ significant human annotation costs.
Table 2: Results of subjective projection using LLM- Table 3: Performance difference (A) between two strate-
as-judge (Judge) and verifiable projection (Verify) of gies for generalized question generation for MCQA: a)
oracle generalized ideation on GPQA-Main. Differ- | Converting GPQA-diamond question stems to be self-
ent projector LLMs are compared: Llama 3.1-8B(L), | contained, and b) using the original question stems.
Gemma 2-9B (G), Qwen 2.5 models (Q-7B, Q-14B, These questions are input to the ideation stage with
Q-32B), and Phi-4 (P). different LLMs: Llama 3.1-8B(L), Gemma 2-9B(G),
Qwen 2.5 models(Q-7B, Q-14B, Q-32B), and Phi-4(P).
Judge 65.6 95.3 70.1 924 973 93.1
cess to additional knowledge of the correct answer 46 Effect of varying projector LLM
to the original question to subjectively evaluate
the oracle. We see that the judge performs poorly Table 4: Performance Gap between the highest scoring
for several LMs (It is even poorer when it doesn’t —_ projector LLM and the lowest scoring projector LLM
have access to original question’s answer — full for both subjective projector (Ajyage) and verifiable pro-
table: Appendix table 8). This indicates that sub- jector (Averify) settings on ARC-challenge (A), GPQA-
jective evaluation techniques like LLM-as-a-judge main (M) and diamond (D) datasets. The ideation stage
do not faithfully reflect the assessment of problem- is reported under different LLMs: Llama 3.1-8B(L),
. an Gemma 2-9B (G), Qwen 2.5 models (Q-7B, Q-14B,
solving capabilities of models. Similar concerns Q-32B), and Phi-4 (P).
about this evaluation paradigm have been raised in
prior work (Panickssery et al., 2024; Zheng et al.,
2023; Dubois et al., 2025; Zhu et al., 2025; Li et al.,
2023; Manakul et al., 2023; Doddapaneni et al., Averity (M) 4.7 3.1 2.2 3.6 5.8 42
2024). In contrast, the verifiable projector achieves Ajudge(D) 11.6 23.7 12.1 19.19 15.1 17.7
near-perfect accuracy across all models, reaching
over 99% in several cases. This validates the cor-
rectness of our verifiable projection approach as it A verify(A) 73 7.6 69 6.7 69 66
behaves almost flawlessly with oracle generalized
ideation. We have shown in 4.4 that verifiable projec-
tors consistently outperform subjective projectors
4.5 Scalability of Our MCQA Instantiation when ground truth explanations are used as ideation
One potential concern in our implementation of — traces. To further compare the robustness of sub-
MCQA instantiation of cascaded information dis- jective and verifiable projectors, we compute the
7


--- Page 8 ---

accuracy gap between the best projector and the — small models and doesn’t affect the performance
worst projector when different LLM projectors are of larger models, thereby increasing the perfor-
used to score the ideation traces from the same mance gap between the models. This suggests that
models. From the results table 4 we can see that — cascaded information disclosure performance isn’t
this gap is consistently larger for subjective pro- merely an artifact of inference-time scaling.
jectors than verifiable projectors. In some cases,
LLM-as-a-Judge based projector performance gap Table 6: Performance gap between Increased-compute
. . . . . : and Standard settings. Two methods of increasing com-
is almost 7 times of verifiable projector, signaling : : .
: _. co , . pute are compared: a) cascaded info disclosure with

extreme instability of the subjective projectors. the ideation LLM as verifiable projector (Agus), and

b) self-reflection (Ajefect) on GPQA-Main (M) and Dia-
4.7 Why LLMs for Verifiable Projection? mond (D). The eat oe is voported wader different

LLMs: Llama 3.1-8B(L), Gemma 2-9B (G), Qwen 2.5
Table 5: Performance difference (A) between two verifi- models (Q-7B, Q-14B, Q-32B), and Phi-4 (P).
able projection strategies: a) using an LLM and, b) using
a rigid rule-based system. The ideation stage is pro-
duced by different LLMs: Llama 3.1-8B(L), Gemma 2- Aours (M) 4.7 -1.8 0.9 -2.0 -7.6 -1.6

1- , on Jamond and math wor
problem (GSM)8K-100 datasets.
Aveflect (D) -2.5 -1.5 -1.0 -15 1.0 -2.5

GPQA A 12.1 -1.0 6.1 6.6 17.2 11.62 . .

Similar to our instantiation, prior work has ex-
Since the verifier projection stage maps the ideation _ plored transforming extant benchmarks for better
stage output to an objective answer, itis possible —_ evaluation. This transformation includes permu-
to build simple heuristics for this stage instead of _ tation of answer choices (Gao et al., 2024), aug-
using an LLM as a projector. For MCQA, the menting an additional choice (Wang et al., 2024),
rule-based projector selects the option that maxi- removing questions from the problem statement
mizes sentenceBLEU with the ideated response. (Balepur et al., 2024), adversarial perturbations of
For GSM8K, we extract a pythonic expression _ problem statements (Li et al., 2024a; Wang et al.,
if available from the tail of the ideated response 2022), translating the questions into another natu-
and execute it for evaluation. Table 5 compares ral language (Yue et al., 2023), formal language
LLM-based projector (Phi-4) to rule-based projec- (Tsoukalas et al., 2024), and programming lan-
tors designed by us (full table in Appendix Table —_ guage (Zheng et al., 2024), among others. Another
11). Across almost all models and datasets, the —_ popular transformation for MCQA is randomly re-
LLM projector substantially outperforms (+ A) the placing the answer choices with None of the
symbolic baseline. Thus, rigid rule-based systems other choices (Balepur et al., 2025; Livingston,
are insufficient for reliable projection, particularly | 2009) We also conducted experiments with this
in tasks requiring nuanced reasoning alignment. transformation (Appendix Table 7a) and observed
On the other hand, this projection task is simple _ that this further shrinks the gap between the models
enough for LLM projectors as shown in § 4.4. as it makes the questions more difficult. Such trans-

. . formations are complementary to our findings and
4.8 Is It Just Inference-time Scaling? can be seamlessly integrated into our framework.
Because, our framework involves more computa- Finally, our evaluation framework is general and
tion steps due to its stagewise nature, in Table 6 — capable of encompassing multiple stages with dis-
we compare it (self as a verifier projector) to a tinct foci. Our observation that our multi-stage
two-round version of the standard evaluation set- _ setup elicits improved intermediate traces suggests
ting (self-reflect). This setting performs the stan- _ a fruitful future direction for learning the decom-
dard QA twice, with the second round also using _ position (Prasad et al., 2024; Renze and Guven,
the model’s response from the first round. We find 2024) and nature of stages for enhanced problem-
that self-reflect behaves in an opposite manner to solving behavior via Reinforcement Learning (RL)
our framework — it hurts the performance on the (DeepSeek-AI et al., 2025; Wen et al., 2025).
8


--- Page 9 ---

6 Limitations answering? try arc, the ai2 reasoning challenge.
Preprint, arXiv:1803.05457.

Although our proposed method provides a more
robust evaluation of a model’s reasoning capabil- Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,
we . . we Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias
ities, it requires some additional compute to run Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
the LLM-based projector. Relatedly, our frame- Nakano, Christopher Hesse, and John Schulman.
work could be extended with recent orthogonal 2021. Training verifiers to solve math word prob-
approaches for inference-time scaling (Wang et al., lems. Preprint, arXiv:2110.14168.

2023), (Madaan et al., 2023), (Snell et al., 2024; a tian M Collins and Albert L Stevens. 1983. A cogni-

Tian et al., 2025). In this work, we focus on a tive theory of interactive teaching. Technical report.

structured and efficient multi-round paradigm that . ;

avoids redundant computation. DeepSeek-Al, Daya Guo, Dejian Yang, Haowei Zhang,

. . Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu,
Furthermore, we restricted our attention to Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang,

MCQA and math word problems in this paper. We Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhi-

did not evaluate on many other tasks of interest hong Shao, Zhuoshu Li, Ziyi Gao, and 181 others.

to the LLM community, such as machine transla- 2025. Deepseek-rl: Incentivizing reasoning capa

. . . bility in ms via reinforcement learning. Preprint,
tion. While our framework is general enough to arXiv:2501.12948.

accommodate multiple tasks and stages, for some

of these cases, formulation and implementation | Sumanth Doddapaneni, Mohammed Safi Ur Rahman

may be nontrivial when instantiating our cascading Khan, Sshubam Verma, and Mitesh M. Khapra. 2024.

; ; : Finding blind spots in evaluator Ilms with inter-

information disclosure framework. For example, —_pyetable checklists. Preprint, arXiv:2406.13439.

some of these cases might require annotating the

dataset as in our GSM-General (although this pro- Yann Dubois, Balazs Galambosi, Percy Liang, and Tat-

: sunori B. Hashimoto. 2025. Length-controlled al-
cess could be automated using an LLM). ee : :
pacaeval: A simple way to debias automatic evalua-
tors. Preprint, arXiv:2404.04475.
References Esther Gan, Yiran Zhao, Liying Cheng, Yancan Mao,
. . ; . . Anirudh Goyal, Kenji Kawaguchi, Min-Yen Kan,

Marah Abdin, Jyoti Aneja, Harkirat Behl, Sébastien and Michael Shieh. 2024. Reasoning robustness of
Bubeck, Ronen Eldan, Suriya Gunasekar, Michael Ilms to adversarial typographical errors. Preprint,
Harrison, Russell J. Hewett, Mojan Javaheripi, Piero arXiv:2411.05345.

Kauffmann, James R. Lee, Yin Tat Lee, Yuanzhi Li,

Weishung Liu, Caio C. T. Mendes, Anh Nguyen, Leo Gao, Jonathan Tow, Baber Abbasi, Stella Bider-

Eric Price, Gustavo de Rosa, Olli Saarikivi, and man, Sid Black, Anthony DiPofi, Charles Foster,

8 others. 2024. Phi-4 technical report. Preprint, Laurence Golding, Jeffrey Hsu, Alain Le Noac’h,

arXiv:2412.08905. Haonan Li, Kyle McDonell, Niklas Muennighoff,
Chris Ociepa, Jason Phang, Laria Reynolds, Hailey

Norah Alzahrani, Hisham Abdullah Alyahya, Yazeed Schoelkopf, Aviya Skowron, Lintang Sutawika, and
Alnumay, Sultan Alrashed, Shaykhah Alsubaie, 5 others. 2024. The language model evaluation har-
Yusef Almushaykeh, Faisal Mirza, Nouf Alotaibi, ness.

Nora Altwairesh, Areeb Alowisheq, M Saiful Bari,

and Haidar Khan. 2024. When benchmarks are tar- Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri,

gets: Revealing the sensitivity of large language Abhinav Pandey, Abhishek Kadian, Ahmad AlI-

model leaderboards. Preprint, arXiv:2402.01781. Dahle, Aiesha Letman, Akhil Mathur, Alan Schel-
ten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh

Nishant Balepur, Abhilasha Ravichander, and Rachel Goyal, Anthony Hartshorn, Aobo Yang, Archi Mi-
Rudinger. 2024. Artifacts or abduction: How do tra, Archie Sravankumar, Artem Korenev, Arthur
Ilms answer multiple-choice questions without the Hinsvark, and 542 others. 2024. The llama 3 herd of
question? Preprint, arXiv:2402.12483. models. Preprint, arXiv:2407.21783.

Nishant Balepur, Rachel Rudinger, and Jordan Lee Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
Boyd-Graber. 2025. Which of these best describes Mantas Mazeika, Dawn Song, and Jacob Steinhardt.
multiple choice evaluation with llms? a) forced 2021. Measuring massive multitask language under-
b) flawed c) fixable d) all of the above. Preprint, standing. Preprint, arXiv:2009.03300.
arXiv:2502.14127.

Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian- Yun

Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Nie, and Ji-Rong Wen. 2023. Halueval: A large-
Ashish Sabharwal, Carissa Schoenick, and Oyvind scale hallucination evaluation benchmark for large
Tafjord. 2018. Think you have solved question language models. Preprint, arXiv:2305.11747.

9


--- Page 10 ---

Qintong Li, Leyang Cui, Xueliang Zhao, Lingpeng = Archiki Prasad, Alexander Koller, Mareike Hartmann,
Kong, and Wei Bi. 2024a. Gsm-plus: A compre- Peter Clark, Ashish Sabharwal, Mohit Bansal, and
hensive benchmark for evaluating the robustness of Tushar Khot. 2024. Adapt: As-needed decomposi-
Ilms as mathematical problem solvers. Preprint, tion and planning with language models. Preprint,
arXiv:2402.19255. arXiv:2311.05772.

Yucheng Li, Frank Guerin, and Chenghua Lin. 2024b. Quwen, :, An Yang, Baosong Yang, Beichen Zhang,
An open source data contamination report for large Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan
language models. Preprint, arXiv:2310.17589. Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan

Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin

Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Yang, Jiaxi Yang, Jingren Zhou, and 25 oth-

Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian ers. 2025. Qwen2.5 technical report. Preprint,
Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Ku- arXiv:2412.15115.
mar, Benjamin Newman, Binhang Yuan, Bobby Yan, ; . :
Ce Zhang, Christian Cosgrove, Christopher D. Man- Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev,
ning, Christopher Ré, Diana Acosta-Navas, Drew A. and Percy Liang. 2016. Squad: 100,000+ ques-
Hudson, and 31 others. 2023. Holistic evaluation of tions for machine comprehension of text. Preprint,
language models. Preprint, arXiv:2211.09110. arXiv: 1606.05250.

Stephanie Lin, Jacob Hilton, and Owain Evans. 2022. David Rein, Betty Li Hou, Asa Cooper Stickland,
Truthfulga: Measuring how models mimic human Jackson Petty, Richard Yuanzhe Pang, Julien Di-
falsehoods. Preprint, arXiv:2109.07958. rani, Julian Michael, and Samuel R. Bowman. 2023.

Gpqa: A graduate-level google-proof q&a bench-

Samuel A. Livingston. 2009. Constructed-response test mark. Preprint, arXiv:2311.12022.
questions: Why we use them; how we score them. Matthew Renze and Erhan Guven. 2024. The benefits
r&d connections. number 11. Educational Testing ‘ . oo
Service. of a concise chain of thought on problem-solving in

large language models. In 2024 2nd International

Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Conference on Foundation and Large Language Moda-
Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, els (FLLM), page 476-483. IEEE.

Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Paul-Edouard Sarlin, Daniel DeTone, Tomasz Mal-

Shashank Gupta, Bodhisattwa Prasad Majumder, isiewicz, and Andrew Rabinovich. 2020. Superglue:

Katherine Hermann, Sean Welleck, Amir Yazdan- Learning feature matching with graph neural net-

bakhsh, and Peter Clark. 2023. Self-refine: Ite orks, Preprint, arXiv:1911.11763.

erative refinement with self-feedback. Preprint,

arXiv:2303.17651. Rulin Shao, Shuyue Stella Li, Rui Xin, Scott Geng,
. i. Yiping Wang, Sewoong Oh, Simon Shaolei Du,

Potsawee Manakul, Adian Liusie, and Mark J. F. Gales. Nathan Lambert, Sewon Min, Ranjay Krishna,
2023. Selfcheckgpt: Zero-resource black-box hal- Yulia Tsvetkov, Hannaneh Hajishirzi, Pang Wei
lucination detection for generative large language Koh, and Luke Zettlemoyer. 2025. Spurious re-
models. Preprint, arXiv:2303.08896. wards: Rethinking training signals in rlvr. Preprint,

. . . . arXiv:2506.10947.

Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi,

Oncel Tuzel, Samy Bengio, and Mehrdad Farajtabar. Charlie Snell, Jachoon Lee, Kelvin Xu, and Aviral Ku-
2024. Gsm-symbolic: Understanding the limitations mar. 2024. Scaling llm test-time compute optimally
of mathematical reasoning in large language models. can be more effective than scaling model parameters.
Preprint, arXiv:2410.05229. Preprint, arXiv:2408.03314.

OpenAI, :, Aaron Hurst, Adam Lerer, Adam P. Goucher, Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao,
Adam Perelman, Aditya Ramesh, Aidan Clark, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch,
AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Adam R. Brown, Adam Santoro, Aditya Gupta,
Radford, Aleksander Madry, Alex Baker-Whitcomb, Adria Garriga-Alonso, Agnieszka Kluska, Aitor
Alex Beutel, Alex Borzunov, Alex Carney, Alex Lewkowycz, Akshat Agarwal, Alethea Power, Alex
Chow, Alex Kirillov, and 401 others. 2024. Gpt-4o Ray, Alex Warstadt, Alexander W. Kocurek, Ali
system card. Preprint, arXiv:2410.21276. Safaya, Ali Tazarv, and 432 others. 2023. Be-

yond the imitation game: Quantifying and extrap-

Arjun Panickssery, Samuel R. Bowman, and Shi Feng. olating the capabilities of language models. Preprint,
2024. Llm evaluators recognize and favor their own arXiv:2206.04615.
generations. Preprint, arXiv:2404.13076.

Gemma Team, Morgane Riviere, Shreya Pathak,

Matt Post. 2018. A call for clarity in reporting BLEU Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupati-
scores. In Proceedings of the Third Conference on raju, Léonard Hussenot, Thomas Mesnard, Bobak
Machine Translation: Research Papers, pages 186— Shahriari, Alexandre Ramé, Johan Ferret, Peter Liu,
191, Brussels, Belgium. Association for Computa- Pouya Tafti, Abe Friesen, Michelle Casbon, Sabela
tional Linguistics. Ramos, Ravin Kumar, Charline Le Lan, Sammy

10


--- Page 11 ---

Jerome, and 179 others. 2024. Gemma 2: Improving —_Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan
open language models at a practical size. Preprint, Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
arXiv:2408.00118. Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang,
Joseph E. Gonzalez, and Ion Stoica. 2023. Judg-
Xiaoyu Tian, Sitong Zhao, Haotian Wang, Shuaiting ing llm-as-a-judge with mt-bench and chatbot arena.
Chen, Yunjie Ji, Yiping Peng, Han Zhao, and Xian- Preprint, arXiv:2306.05685.
gang Li. 2025. Think twice: Enhancing Ilm rea-
soning by scaling multi-round test-time thinking. Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan
Preprint, arXiv:2503.19855. Wang, Yufei Xue, Zihan Wang, Lei Shen, Andi Wang,
Yang Li, Teng Su, Zhilin Yang, and Jie Tang. 2024.
George Tsoukalas, Jasper Lee, John Jennings, Jimmy Codegeex: A pre-trained model for code genera-
Xin, Michelle Ding, Michael Jennings, Amitayush tion with multilingual benchmarking on humaneval-x.
Thakur, and Swarat Chaudhuri. 2024. Putnam- Preprint, arXiv:2303.17568.
bench: Evaluating neural theorem-provers on
the putnam mathematical competition. Preprint, | Lianghui Zhu, Xinggang Wang, and Xinlong Wang.
arXiv:2407.11214. 2025. Judgelm: Fine-tuned large language models
are scalable judges. Preprint, arXiv:2310.17631.
Alex Wang, Amanpreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel R. Bowman. 2019.
Glue: A multi-task benchmark and analysis plat-
form for natural language understanding. Preprint,
arXiv:1804.07461.
Boxin Wang, Chejian Xu, Shuohang Wang, Zhe Gan,
Yu Cheng, Jianfeng Gao, Ahmed Hassan Awadal-
lah, and Bo Li. 2022. Adversarial glue: A multi-
task benchmark for robustness evaluation of language
models. Preprint, arXiv:2111.02840.
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,
Ed Chi, Sharan Narang, Aakanksha Chowdhery, and
Denny Zhou. 2023. Self-consistency improves chain
of thought reasoning in language models. Preprint,
arXiv:2203.11171.
Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni,
Abhranil Chandra, Shiguang Guo, Weiming Ren,
Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max
Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue,
and Wenhu Chen. 2024. Mmlu-pro: A more robust
and challenging multi-task language understanding
benchmark. Preprint, arXiv:2406.01574.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and
Denny Zhou. 2023. Chain-of-thought prompting elic-
its reasoning in large language models. Preprint,
arXiv:2201.11903.
Xumeng Wen, Zihan Liu, Shun Zheng, Zhijian Xu,
Shengyu Ye, Zhirong Wu, Xiao Liang, Yang Wang,
Junjie Li, Ziming Miao, Jiang Bian, and Mao Yang.
2025. Reinforcement learning with verifiable re-
wards implicitly incentivizes correct reasoning in
base Ilms. Preprint, arXiv:2506. 14245.
David Wood, Jerome S Bruner, and Gail Ross. 1976.
The role of tutoring in problem solving. Journal of
child psychology and psychiatry, 17(2):89-100.
Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wen-
hao Huang, Huan Sun, Yu Su, and Wenhu Chen.
2023. Mammoth: Building math generalist mod-
els through hybrid instruction tuning. Preprint,
arXiv:2309.05653.
11


--- Page 12 ---

7 Appendix
12


--- Page 13 ---

"original_question”: "Brett is 14 years old. In four years his sister Angela will be

three times as old as he is now. How old is Angela right now?”,

"answer_numeric”: 38,

"generalized_symbolic_question”: "{namel1} is {agel} years old. In {years} years his
{relation_type} {name2} will be {mult} times as old as {namel} is now. How old
is {name2} right now?\n\nVariables and the range of possible values they can
take are:\n- namel = sample(names_male)\n- name2 = sample(names_female) \n-
relation_type = sample(['sister', 'cousin'])\n- $agel = range(8, 25)\n- $years =

range(2, 10)\n- $mult = range(2, 5)\n\nThe relationship variables should have
is:\n- agel * mult - years > @\n\n",

"answer_symbolic” : "agelxmult-years”,

"ideation_traces": "<Reasoning>\n1l. Let's denote the current age of {name2} as {
age2}.\n2. In {years} years, {name1} will be {agel} + {years} years old.\n3.

In {years} years, {name2} will be {age2} + {years} years old.\n4. We know that
in {years} years, {name2} will be {mult} times as old as {name1} is now, so we
can write the equation: {age2} + {years} = {mult} * {agel}\n5. We need to solve

this equation for {age2}.\n\n</Reasoning >\n<Answer >{age2} = {mult} * {agel} - {
years}</Answer>",

"raw_symbolic_answer"”: "{age2} = {mult} * {agel} - {years}",

"cleaned_symbolic_answer”: "(mult * agel) - years”,

"projected_numeric_result”: 38,

"Imh_answer_trace”: "We need to figure out how old Angela is right now. We know
that in four years her sister Angela will be three times as old as he is now.

We also know that Angela is right now. We need to figure out how old Angela is
right now.\n\n\nLet me know if you have any other questions.<end_of_turn>”,

"Imh_filtered_answer"”: "[Linvalid]”

Listing 1: GSM8K Example. Evaluation subject is Gemma-2-9B

7.1 Qualitative Examples

7.1.1 Example from GSM8K Dataset

13


--- Page 14 ---

"id": "996",
"question”: "Which of these is not part of an atom?”,
"options”: [

"isotope”,

"proton”,

"nucleus”,

"electron”
J ’
"answer_index”: @,
"Phi-4_raw_output”: "<Reasoning>\nTo determine which option is not part of an
atom, let's analyze each choice:\n\nA) Isotope: An isotope refers to variants of

a particular chemical element that have the same number of protons but
different numbers of neutrons. Isotopes are not a part of an atom itself; rather
, they are different forms of atoms of the same element. Therefore, isotopes are

not a component of a single atom.\n\nB) Proton: Protons are positively charged
particles found in the nucleus of an atom. They are a fundamental part of the
atom's structure.\n\nC) Nucleus: The nucleus is the central part of an atom,
containing protons and neutrons. It is a crucial component of the atom.\n\nD)
Electron: Electrons are negatively charged particles that orbit the nucleus of
an atom. They are essential parts of the atom's structure.\n\nBased on this
analysis, the option that is not a part of an atom is A) isotope, as it refers
to different forms of atoms rather than a component of a single atom.\n</
Reasoning >\n<PickedAnswer >A</PickedAnswer >”,
"is_correct”: "Yes”

Listing 2: ARC-Challenge Example: Phi-4 Correctly Answers Question in Standard MCQA Setting

7.1.2 Example from ARC-Challenge Dataset
14


--- Page 15 ---

"id": "996",

"phi-4_ideation_traces”: "To determine which component is not part of an atom, we
need to understand the basic structure of an atom. An atom is the smallest unit
of ordinary matter that forms a chemical element. It consists of three primary

subatomic particles:\n\n1l. **Protons**: These are positively charged particles

found in the nucleus of an atom. The number of protons in the nucleus defines

the atomic number of an element and determines the element's identity.\n\n2. xx

Neutronsx*x: These are neutral particles, meaning they have no charge, and are

also located in the nucleus alongside protons. Neutrons contribute to the mass

of the atom and can influence the stability of the nucleus.\n\n3. **Electrons*x:
These are negatively charged particles that orbit the nucleus in various energy
levels or electron shells. Electrons are much smaller in mass compared to
protons and neutrons and are responsible for the chemical properties of an atom
-\n\nGiven this understanding, any component not fitting into these categories (
protons, neutrons, or electrons) would not be considered part of an atom.

Commonly, other particles or structures that are not part of an atom include:\n\

n- *xMolecules**: These are groups of two or more atoms bonded together.

Molecules are not part of a single atom but rather a collection of atoms.\n\n-

*xkIons**: These are atoms or molecules that have gained or lost electrons,

resulting in a net charge. While ions are related to atoms, they are not a

fundamental part of an atom's structure.\n\n- *xNucleus**x: While the nucleus is

a part of an atom, it is not a subatomic particle itself but rather a structure

within the atom that contains protons and neutrons.\n\n- **Quarks**: These are

fundamental particles that make up protons and neutrons. While quarks are part
of the subatomic particles, they are not considered part of the atom itself in
the context of basic atomic structure.\n\n- **xPhotons**: These are particles of
light and do not form part of an atom. They are involved in electromagnetic
interactions but are not components of atomic structure.\n\nBased on this
analysis, the correct answer would be a component that does not fit into the
categories of protons, neutrons, or electrons, such as a molecule, ion, nucleus,
quark, or photon.”,

"phi-4_ideation_stage_answer”: "A molecule, ion, nucleus, quark, or photon is not
part of an atom.”,

"verifiable_projector_raw_output”: "<Reason>\nThe key conclusions from the '

Reasoning Trace' are that an atom consists of three primary subatomic particles:
protons, neutrons, and electrons. Any component that does not fit into these

categories is not considered part of an atom. The trace specifically identifies

molecules, ions, the nucleus, quarks, and photons as examples of components that
are not part of an atom.\n\nNow, let's evaluate each answer option:\n\n- «x

Option A: protonx*x\n - Protons are explicitly mentioned in the reasoning trace

as a fundamental component of an atom. Therefore, this option does not match the
conclusion that the component is not part of an atom.\n\n- *xOption B: electron

xx\n - Electrons are also explicitly mentioned in the reasoning trace as a

fundamental component of an atom. Thus, this option does not match the

conclusion that the component is not part of an atom.\n\n- *xOption C: nucleus

xx\n - The reasoning trace states that the nucleus is not a subatomic particle

itself but a structure within the atom that contains protons and neutrons.

Therefore, the nucleus is not considered a fundamental component of an atom in

the context of subatomic particles. This aligns with the conclusion that the

component is not part of an atom.\n\n- **xOption D: isotopex*x\n - Isotopes are
variants of a particular chemical element that differ in neutron number. While
isotopes are related to atoms, they are not a fundamental part of an atom's
structure. However, the reasoning trace does not explicitly mention isotopes as
an example of a component not part of an atom. The focus is more on subatomic
particles and structures like the nucleus.\n\nBased on the analysis, Option C (
nucleus) is the most consistent and closest match to the reasoning trace, as it

is explicitly identified as not being a subatomic particle and thus not a

fundamental component of an atom in the context of the trace.\n</Reason>\n<

PickedAnswer >C</PickedAnswer >",

"projected_answer_text”: “nucleus”,

"is_anwer_correct”: "No”

Listing 3: ARC-Challenge Example: Phi-4 Fails to Derive the Correct Answer in Cascaded Information Disclosure
Setting
15


--- Page 16 ---

7.1.3 Example from GPQA Dataset
Qwen2.5-7B’s answer under standard MCQA Evaluation
16


--- Page 17 ---

"resps”: "<Reasoning>\nTo solve this problem, we need to understand the
Kraus representation of a quantum channel and how it relates to the given Kraus
operators. The Kraus operators \\(A_i\\) for a depolarizing channel are given as
:\n\n\\— A_@ = \\sqrt{1 - \\frac{3p}{4}} I \\IJ\n\\E A_1 = \\sqrt{\\frac{p}{4}} xX

\\J\n\\E AL2 = \\sqrt{\\frac{p}{4}} Y \\IJ\n\\E A_3 = \\sqrt{\\frac{p}{4}} Z
\\J\n\nThe depolarizing channel can be represented as:\n\n\\[ E(\\rho) = \\sum_i
A_i \\rho A_i*\\dagger \\]\n\nGiven the Kraus operators, we can substitute them
into the equation:\n\n\\[ E(\\rho) = A_@ \\rho A_@*\\dagger + A_1 \\rho A_1%*\\
dagger + A_2 \\rho A_2%\\dagger + A_3 \\rho A_3*\\dagger \\J]\n\nSince \\(A_@ =
\\sqrt{1 - \\frac{3p}{4}} I\\), we have:\n\n\\—E A_@ \\rho A_@*\\dagger = (1 - \\
frac{3p}{4}) \\rho \\J\n\nFor the other operators, we have:\n\n\\[ A_1 \\rho A_1
“\\dagger = \\frac{p}{4} X \\rho X \\J\n\\E A_2 \\rho A_2*\\dagger = \\frac{p
3{4} Y \\rho Y \\J\n\\L A_3 \\rho A_3*\\dagger = \\frac{p}{4} Z \\rho Z \\J]\n\
nCombining these, we get:\n\n\\[ E(\\rho) = (1 - \\frac{3p}{4}) \\rho + \\frac{p
3{4} X \\rho X + \\frac{p}{4} Y \\rho Y + \\frac{p}{4} Z \\rho Z \\J\n\nWe can
rewrite this as:\n\n\\[ E(\\rho) = (1 - \\frac{3p}{4}) \\rho + \\frac{p}{4} (xX
\\rho X + Y \\rho Y + Z \\rho Z) \\J]\n\nWe know that \\(X*%2 = Y*2 = Z*2 = I\\),
so:\n\n\\C X \\rho X = \\rho \\J\n\\E Y \\rho Y = \\rho \\J\n\\E Z \\rho Z = \\
rho \\J\n\nThus, the expression simplifies to:\n\n\\[L E(\\rho) = (1 - \\frac{3p
3{4}) \\rho + \\frac{p}{4} (\\rho + \\rho + \\rho) = (1 - \\frac{3p}{4}) \\rho +
\\frac{3p}{4} \\rho = (1 - \\frac{3p}{4} + \\frac{3p}{4}) \\rho = (1 - p) \\rho
+ \\frac{p}{3} (X \\rho X + Y \\rho Y + Z \\rho Z) \\J\n\nThis matches the form
given in option A.\n\nTherefore, the correct Kraus representation of the state
\\CEQ\\rho)\\) is:\n\n\\E EC\\rho) = (1 - p) \\rho + \\frac{p}{3} (X \\rho X + Y
\\rho Y + Z \\rho Z) \\J]\n\n</Reasoning >\n<PickedAnswer >A</PickedAnswer >”

Listing 4: Qwen Custom Example
17


--- Page 18 ---

"reasoning”: "To derive the Kraus representation of the depolarizing channel
operation \( E(\rho) \), we need to ensure that the sum of the squares of the
Kraus operators equals the identity matrix. The given Kraus operators are:\n\n\L

A_®@ = \sqrt{1 - \frac{3p}{4}} I \J\n\E A_1 = \sqrt{\frac{p}{4}} X \IJ\n\E A_2 =
\sqrt{\frac{p}{4}} Y \J\n\[— A_3 = \sqrt{\frac{p}{4}} Z \]\n\nFirst, let's verify
that the sum of the squares of these operators equals the identity matrix \( I

\)e\n\n\— A_@*2 + A_142 + A_2%2 + A_342 = \left( \sqrt{1 - \frac{3p}{4}} I \
right)*2 + \left( \sqrt{\frac{p}{4}} X \right)*2 + \left( \sqrt{\frac{p}{4}} Y \
right)*2 + \left( \sqrt{\frac{p}{4}} Z \right)*2 \]\n\nCalculating each term
individually:\n\n\[L A_0*2 = \left( \sqrt{1 - \frac{3p}{4}} I \right)*2 = \left(
1 - \frac{3p}{4} \right) I \J\n\[E A_1%2 = \left( \sqrt{\frac{p}{4}} X \right)%*2
= \frac{p}{4} X*2 = \frac{p}{4} I \J\n\E A_2*2 = \left( \sqrt{\frac{p}{4}} Y \
right)*2 = \frac{p}{4} Y*%2 = \frac{p}{4} I \]\n\[E A_3%2 = \left( \sqrt{\frac{p
3{4}} Z \right)*2 = \frac{p}{4} Z*2 = \frac{p}{4} I \J]\n\nSumming these up:\n\n
\D A_@*2 + A_1%2 + A_2%2 + A_3%2 = \left( 1 - \frac{3p}{4} \right) I + \frac{p
3{4} I + \frac{p}{4} I + \frac{p}{4} I \J\n\0E = \left¢ 1 - \frac{3p}{4} + \fracf{
p}{4} + \frac{p}{4} + \frac{p}{4} \right) I \J\n\C = \left( 1 - \frac{3p}{4} + \
frac{3p}{4} \right) I \J\n\C = 1 I \J\n\C = I \J\n\nSince the sum of the squares
of the Kraus operators equals the identity matrix, the given Kraus operators
correctly represent the depolarizing channel operation \( E(\rho) \).\n\
nTherefore, the correct Kraus representation of the state \( E(\rho) \) is:\n\n
\C E(\rho) = \sum_{i=0}*{3} A_i \rho A_i*\dagger \]\n\nWhere the Kraus operators
are:\n\n\[L A_® = \sqrt{1 - \frac{3p}{4}} I \J\n\E A_1 = \sqrt{\frac{p}{4}} xX
\J\n\0C A_2 = \sqrt{\frac{p}{4}} Y \J\n\0 A_3 = \sqrt{\frac{p}{4}} Z \]",

"answer”: "\[ E(\rho) = A_@ \rho A_@*\dagger + A_1 \rho A_1*\dagger + A_2 \rho A_2*\
dagger + A_3 \rho A_3*\dagger \]\nWhere \( A_® = \sqrt{1 - \frac{3p}{4}} I \),
\C A_1 = \sqrt{\frac{p}{4}} X \), \C A_2 = \sqrt{\frac{p}{4}} Y \), and \( A_3 =

\sqrt{\frac{p}{4}} Z \).”
Listing 5: Qwen Ideation Traces
Qwen2.5-7B’s ideation traces under our framework:
18


--- Page 19 ---

"ideation_traces”: "<Reason>\nThe reasoning trace provides a detailed derivation of
the Kraus representation for a depolarizing channel. The key conclusions from
the reasoning trace are:\n\n1l. The Kraus operators for the depolarizing channel
are:\n - \C A_® = \sqrt{1 - \frac{3p}{4}} I \)\n - \C A_1 = \saqrt{\frac{p
3{4}} X \)\n - \C A_2 = \sqrt{\frac{p}{4}} Y \)\n - \( A_3 = \sqrt{\frac{p
3{4}} Z \)\n\n2. The sum of the squares of these Kraus operators equals the
identity matrix, confirming they form a valid representation.\n\n3. The
depolarizing channel operation \( E(\rho) \) is expressed as:\n \O\n E(\rho)
= A_®@ \rho A_@*\dagger + A_1 \rho A_1*\dagger + A_2 \rho A_2*\dagger + A_3 \rho
A_3*\dagger\n \]\n\n4. Substituting the Kraus operators into the expression
for \( E(\rho) \), we get:\n \O\n E(\rho) = \left(1 - \frac{3p}{4}\right) \
rho + \frac{p}{4} X \rho X + \frac{p}{4} Y \rho Y + \frac{p}{4} Z \rho Z\n \I\
n\nNow, let's evaluate each answer option:\n\n- *xOption A:**\n \C[\n E(\rho) =
(1-p)\rho + \frac{p}{3}X\rho*{2}X + \frac{p}{3}Y¥\rho*{2}Y + \frac{p}{3}Z\rho
“{2}Z\n \]\n This option uses \(\rho*2\) instead of \(\rho\) and has
coefficients \(\frac{p}{3}\) instead of \(\frac{p}{4}\). It does not match the
derived expression.\n\n- **Option B:**\n \[\n E(\rho) = (1-p)\rho + \fracf{p
3{4}X\rho X + \frac{p}{4}Y¥\rho Y + \frac{p}{4}Z\rho Z\n \]\n This option has
the correct form and coefficients \(\frac{p}{4}\) for the terms involving \(X\),
\CY\), and \(Z\). However, the coefficient for the identity term is \((1-p)\)
instead of \(\left(1 - \frac{3p}{4}\right)\). Despite this discrepancy, it is
the closest match in terms of structure and coefficients.\n\n- *xOption C:*x*\n
\E\n E(\rho) = (1-p)\rho + \frac{p}{3}X\rho X + \frac{p}{3}Y¥\rho Y + \frac{p
3{3}Z\rho Z\n \J\n_ This option uses \(\frac{p}{3}\) instead of \(\frac{p}{4}\)
and \((1-p)\) instead of \(\left(1 - \frac{3p}{4}\right)\). It does not match
the derived expression.\n\n- **xOption D:**\n \[\n E(\rho) = (1-p)\rho*{2} + \
frac{p}{3}X\rho*{2}X + \frac{p}{3}Y¥\rho*{2}¥ + \frac{p}{3}Z\rho*{2}Z\n \I\n
This option uses \(\rho*2\) instead of \(\rho\) and has incorrect coefficients
\(\frac{p}{3}\). It does not match the derived expression.\n\nOverall, Option B
is the closest match to the reasoning trace in terms of structure and
coefficients, despite the slight discrepancy in the coefficient of the identity
term. \n</Reason>\n<PickedAnswer >B</PickedAnswer >"
Listing 6: projector=phi4
The projection traces in stage 2 - Phi-4 as projector:
7.2 Prompt Templates and XML Tag Based Answer Extraction
7.2.1 Prompt Templates
Prompt used in our custom implementation of MCQA Evaluation
Prompt used for self-reflection
Prompt used in ideation stage Prompt used by verifiable projector Prompt used by Subjective
LLM Projector
7.2.2 XML Tag Based Answer Extraction
As we shown above, we adopt a XML tag based prompting strategies where each functional components
are wrapped with XML tag that summarizes its function. We also include clear instructions and a
demonstration for the desired output XML format at the end. Then in the answer extraction stage,
we use regular expression to search for the corresponding pattern (e.g. for answers, we are searching
for <PickedAnswer> and </PickedAnswer>). In our experiments, we found that this method greatly
improves the stability of models instruction following performance in answer format and results in a near
zero parsing failure rates.
7.3. Additional Experimental Results
19


--- Page 20 ---

You are a helpful assistant. You will be provided with a challenging science domain
question (e.g., biology, physics, chemistry) and a set of multiple-choice
options. Your task is to reason through the question step-by-step to arrive at a

detailed answer and select the correct option.

Follow these steps:

1. Think step-by-step to analyze the question and derive a solution. Detail your
reasoning process.

2. After your reasoning, clearly state your final answer by picking the correct
option letter.

Output your response using the following XML format ONLY:

<Reasoning>

Your detailed step-by-step thinking process here.

</Reasoning>

<PickedAnswer >X</PickedAnswer> (where X is the letter of the correct option)

Listing 7: Custom Implementation of Standard MCQA Evaluation Prompt

You are an expert evaluator. You will be provided with a ‘Reasoning Trace' from a

language model that attempted to answer a question and multiple ‘Answer Options'
(labeled as A, B, C, etc.). The original question will also be provided.

Your task is to critically re-evaluate the entire problem to find the correct answer
. You must perform your own independent, step-by-step analysis to arrive at the
most accurate solution, using the provided ‘Reasoning Trace' as a reference but
not assuming it is correct.

Consider the following when making your decision:

S *xxNumerical Closeness:** If the options and your reasoning involve numerical
values, convert all relevant numbers to scientific notation (e.g., expressing
numbers as a coefficient multiplied by 10 raised to an exponent, like 1.23 x
10*°4 or 5.6 x 10*-2). Ensure all numbers are in this consistent format, and in
the same unit if applicable, before comparison. Then, identify the option whose
scientific notation value is closest to the scientific notation value derived
from your reasoning.

S *kConceptual Closeness:** If the options involve concepts, equations,
definitions, or qualitative explanations, identify the option that best reflects

the core ideas in your reasoning or is logically inferred from your reasoning.

Follow these steps:

1. Carefully read and understand the original question, all options, and the entire

"Reasoning Trace'.

2. Conduct your own detailed, step-by-step reasoning process to solve the problem.
You can reference, correct, or build upon the provided trace.

3. Based on your independent analysis, decide which option is the most consistent
and closest match.

Output your evaluation using the following XML format ONLY:

<Reasoning>

Your detailed step-by-step reasoning here.

</Reasoning>

<PickedAnswer >X</PickedAnswer> (where X is the letter corresponding to your chosen
option)

Listing 8: Self Reflection Prompt
20


--- Page 21 ---

You are a helpful assistant. You will receive a challenging science-domain question,
but the multiple-choice options are hidden from you. Your step-by-step
scientific reasoning will be passed to a Stage-2 AI that can see the options and
will rely on your analysis to choose the correct one.

Therefore:

1. Do NOT refuse, disclaim, or say that the task is impossible because the options
are missing.

2. Apply rigorous scientific reasoning: cite relevant concepts, principles,
equations, variables, and alternative scenarios; deduce the most plausible
answer or the exact characteristics the correct answer must have.

3. Do NOT quote, paraphrase, or repeat the question text in your response.

OUTPUT FORMAT --- STRICT

Respond using EXACTLY the XML structure shown below and nothing else.

Formatting rules:

- Put your full, detailed reasoning inside the <Reasoning> ... </Reasoning> tags.

- Put your concise final answer (or the uniquely identifying properties of the
correct answer) inside the <Answer> ... </Answer> tags.

- Do not include any text before <Reasoning> or after </Answer>.

- Do not add extra tags, attributes, or wrapper elements.

<Reasoning>Your step-by-step thinking process goes here.</Reasoning>

<Answer>Your final derived answer goes here.</Answer>

Listing 9: Ideation Stage Prompt

You are an expert evaluator whose ONLY goal is to select the best answer OPTION (A,
B, C, ...) based on a provided Reasoning Trace.

What you receive

- The Reasoning Trace: a step-by-step analysis already produced by another model.

- The Answer Options: labelled A, B, C, ... (the original question text may be
omitted).

Your responsibilities

1. DO NOT solve the original question again. Judge options solely by how well they
align with the Reasoning Trace's conclusions.

2. DO NOT refuse, claim uncertainty, or state that none of the options match. You
MUST pick the single closest option even if the match is imperfect.

3. Use two dimensions of closeness:

- Numerical closeness: convert every relevant number in both trace and options to
scientific-notation in the SAME UNITS, then choose the option with the smallest
numerical difference (consider both coefficient and exponent).

- Conceptual closeness: match the key ideas, equations, or qualitative statements
in the trace.

4. If several options tie, pick the first among the tied ones in alphabetical order
and explain the tie-break.

OUTPUT FORMAT --- STRICT (XML)

You MUST comply with this exact structure; any deviation will cause automatic
failure.

Rules

- Put your comparative analysis inside <Reason> ... </Reason>.

- End with a single uppercase letter (A, B, C, ...}) inside <PickedAnswer> ... </
PickedAnswer >.

- NEVER leave <PickedAnswer> blank, use words, or add extra tags/attributes.

- Do NOT output anything before <Reason> or after </PickedAnswer >.

<Reason>Your detailed comparison of each option to the Reasoning Trace goes here.
Begin with a one-sentence summary of the trace's conclusion, then analyse each
option (A, B, C...) explaining its fit or mismatch. Finish by justifying the
option you will pick.</Reason>

<PickedAnswer >X</PickedAnswer >

Listing 10: Verifiable Projector Prompt
21


--- Page 22 ---

You are an expert evaluator. You will be provided with a ‘Reasoning Trace' from a

language model that attempted to answer a question. The original question may be
provided, and in some cases, the correct answer may also be provided for
reference.

Your task is to meticulously analyze the 'Reasoning Trace' and evaluate whether the
reasoning process and conclusions are correct. You must not try to solve the
original question yourself; your evaluation should be based solely on the
logical soundness and accuracy of the reasoning presented.

Consider the following when making your decision:

S *xxkLogical Soundness:** Evaluate if the reasoning follows a clear, logical
progression and if each step is properly justified.

= xxNumerical Accuracy:**x If the reasoning involves numerical calculations, verify

if the math is correct and if the units are properly handled.

S xxkConceptual Accuracy:** If the reasoning involves scientific concepts,
equations, or definitions, verify if they are correctly applied and interpreted.

S *xkCompleteness:**x Check if the reasoning addresses all aspects of the question
and if any assumptions are clearly stated and justified.

Follow these steps:

1. Carefully read and understand the entire ‘Reasoning Trace’.

2. If a reference answer is provided, use it to verify the accuracy of the
reasoning.

3. Identify any logical flaws, incorrect assumptions, or mathematical errors.

4. Based on your analysis, determine if the reasoning is Correct or Incorrect.

Output your evaluation using the following XML format ONLY:

<Reason>

Your detailed step-by-step analysis here. First, summarize the key points in the
reasoning trace. Then, explain any flaws or strengths you identified. Finally,
state your overall justification for your judgment.

</Reason>

<Judgment >Correct </Judgment> or <Judgment>Incorrect </ Judgment >

Listing 11: LLM Judge Projector Prompt
22


--- Page 23 ---

(a) Model accuracy on the GPQA Diamond dataset when the correct answer is swapped out with “None of the other choices.” For
“Phi (No swap)”, we report accuracies for the case when no changes are made to the answer choices. Higher is better. The A row
reports the difference between minimum and maximum for each column. Parsing failure rates are 0% for the projector methods.
Projector
Standard Self | Phi Phi (No swap)
GPQA Diamond - Accuracy (%)
Llama3.1-8B 15.66 28.28 37.37 32.83
Gemma2-9B 13.13. 27.78 37.37 31.82
Qwen2.5-7B 27.78 31.82 41.41 34.85
Qwen2.5-14B 40.40 34.34 41.92 41.92
Phi-4 (14B) 3.03 45.96 45.96 51.52
Qwen2.5-32B 35.35 38.89 43.43 44.95
A 37.37 18.18 8.59 19.7
(b) Parsing failure rates for the ablation studies on the GPQA Diamond dataset. The table compares failure rates when the correct
answer is swapped with a "None" option versus when it is swapped with a random incorrect option.
Projector (Ours)
Model Standard Self Phi Phi (No swap)
GPQA Diamond - Parsing Failure (%)
Llama3.1-8B 29.29 0.00 0.00 0.00
Gemma2-9B 47.47 0.00 0.00 0.00
Qwen2.5-7B 5.56 0.00 0.00 0.00
Qwen2.5-14B 0.51 0.00 0.00 0.00
Phi-4 (14B) 91.41 0.00 0.00 0.00
Qwen2.5-32B 5.56 0.00 0.00 0.00
23


--- Page 24 ---

Table 8: Evaluation of projector models. Models were provided with the ground truth solutions, and were asked to
evaluate them as if they were generated from evaluation subject models. Higher is better. Projector consistency
performs nearly perfectly, far outperforming LLM-as-a-judge. For “Judge w/ Answer", we additionally provided
the projector model with the ground truth answer, to see if it would help improve the judge’s performance. Even in
this case, the projector outperforms it.

Projector Model Judge Judge w/ answer Projector (Ours)

GPQA Diamond - Accuracy of projector (%)

Llama3.1-8B 18.69 64.14 92.93

Gemma2-9B 57.07 95.96 97.47

Qwen?2.5-7B 44.95 68.18 98.48

Qwen?2.5-14B 38.89 91.92 98.99

Phi-4 (14B) 79.80 94.95 98.48

Qwen?2.5-32B 62.12 93.94 99.49

GPQA Main - Accuracy of projector (%)

Llama3.1-8B = 25.00 65.63 95.76

Gemma2-9B 60.49 95.31 95.54

Qwen?2.5-7B 45.31 70.09 97.77

Qwen?2.5-14B 339.06 92.41 99.11

Phi-4 (14B) 83.48 97.32 98.21

Qwen2.5-32B 65.85 93.08 99.33
Table 9: Parsing failure rates (%) for models on GPQA datasets under standard MCQA evaluation and our two stage
instantiation of proposed framework. We observe a complete elimination of parsing failures when Phi-4 is chosen
as projector in verifiable projection stage. LMH+ and Custom are both variants of standard MCQA evaluation with
prompt engineering improvements. * - Phi-4’s extreme parsing failure rate in LMH based evaluation is the result of
missing chat template in LMH framework and Phi-4’s high sensitivity to input format.

Standard MCQA Evaluation Our Method
LMH LMH+ Custom Projector - Phi

GPQA Diamond

Llama3.1-8B 34.34 =—-16.16 29.29 0.00

Gemma2-9B 44.95 19.19 5.56 0.00

Qwen?2.5-7B 4.55 0.51 3.54 0.00

Qwen2.5-14B 0.00 0.00 0.00 0.00

Phi-4 (14B) 89.39* 100.00* 0.51 0.00

Qwen?2.5-32B 3.54 0.51 1.52 0.00

GPQA Main

Llama3.1-8B =. 29.02. 13.84 26.79 0.00

Gemma2-9B = 38.62: 19.20 7.37 0.00

Qwen?2.5-7B 3.13 0.45 2.01 0.00

Qwen?2.5-14B 1.79 0.45 0.67 0.00

Phi-4 (14B) 91.29* 100.00* 0.67 0.00

Qwen2.5-32B 2.68 0.67 0.67 0.00

24


--- Page 25 ---

Table 10: Parsing failure rate for LMH, projector, and LLM-as-a-judge. Lower is better.
* - The projector threw an error because the model’s outputs could not be converted to a valid Python expression.
The other three evaluation methods did not throw an error because they simply marked these outputs as incorrect.
Llama seemed to struggle in this scenario due to its shorter context length.
{ - For a few examples, the projector refused to choose an answer based on the model’s outputs. (See section 4.3.)
Judge
Model LMH Projector (Phi) Self Phi-4
ARC Challenge - Parsing Failure (%)
Llama3.1-8B NA 0.09" 1.37 0.00
Gemma2-9B NA 0.00 0.09 0.09
Qwen?2.5-7B NA 0.00 3.92 0.09
Qwen?2.5-14B NA 0.00 0.00 0.00
Phi-4 (14B) NA 0.00 0.00 0.00
Qwen?2.5-32B NA 0.00 0.00 0.00
GPQA Diamond - Parsing Failure (%)
Llama3.1-8B 34.34 0.00 11.62 1.01
Gemma2-9B = 44.95 0.00 0.00 0.00
Qwen?2.5-7B 4.55 0.00 2.02 0.00
Qwen?2.5-14B 0.00 0.00 1.01 0.00
Phi-4 (14B) 89.39 0.00 0.00 0.00
Qwen?2.5-32B = 33.54 0.00 0.51 0.00
GPQA Main - Parsing Failure (%)
Llama3.1-8B = 29.02 0.00 7.81 0.00
Gemma2-9B —. 38.62 0.00 0.22 0.00
Qwen?2.5-7B 3.13 0.00 2.46 0.00
Qwen?2.5-14B 1.79 0.00 0.00 0.00
Phi-4 (14B) 91.29 0.00 0.00 0.00
Qwen?2.5-32B =. 2.68 0.00 0.45 0.00
GSM8K-100 - Parsing Failure (%)
Llama3.1-8B 0.00 10.00* 4.00 1.00
Gemma2-9B 2.55 0.00 0.00 0.00
Qwen?2.5-7B 0.00 0.00 0.00 0.00
Qwen?2.5-14B —-0..00 0.00 1.00 0.00
Phi-4 (14B) 0.00 0.00 0.00 0.00
Qwen?2.5-32B —-0..00 0.00 0.00 0.00
25


--- Page 26 ---

Table 11: Ablation study on the importance of the projector. Higher is better.
Projector
Model LMH Rule-based Phi-4
GPQA Diamond - Accuracy (%)
Llama3.1-8B 18.69 20.71 32.83
Gemma2-9B 14.65 32.83 31.82
Qwen2.5-7B 29.29 28.79 34.85
Qwen2.5-14B 42.42 35.35 41.92
Phi-4 (14B) 4.04 34.34 51.52
Qwen2.5-32B 41.41 33.33 44.95
GPQA Main - Accuracy (%)
Llama3.1-8B 22.76 22.54 33.04
Gemma2-9B 18.30 33.04 32.59
Qwen2.5-7B 33.48 28.57 30.13
Qwen2.5-14B 38.17 32.37 37.72
Phi-4 (14B) 2.90 35.49 45.31
Qwen2.5-32B 41.74 34.60 40.85
GSM8K-100 - Accuracy (%)
Llama3.1-8B 32.00 30.00 42.00
Gemma2-9B 21.00 59.00 74.00
Qwen2.5-7B 76.00 48.00 70.00
Qwen2.5-14B 80.00 75.00 84.00
Phi-4 (14B) 52.00 91.00 91.00
Qwen2.5-32B 87.00 95.00 95.00
26


--- Page 27 ---

Table 12: Ablation study on the importance of the projector. Failure rates are reported across various models on
three different benchmarks. The methods compared are a baseline (LMH) and two projector-based approaches.
Projector
Model LMH Rule-based Phi-4
GPQA Diamond - Parsing Failure (%)
Llama3.1-8B 34.34 38.89 0.00
Gemma2-9B 44.95 2.02 0.00
Qwen2.5-7B = 4.55 13.13 0.00
Qwen?2.5-14B 0.00 2.02 0.00
Phi-4 (14B) 89.39 1.52 0.00
Qwen?2.5-32B 3.54 0.51 0.00
GPQA Main - Parsing Failure (%)
Llama3.1-8B 29.02 31.47 0.00
Gemma2-9B 38.62 1.56 0.00
Qwen?2.5-7B 3.13 12.95 0.00
Qwen?2.5-14B 1.79 1.12 0.00
Phi-4 (14B) 91.29 2.01 0.00
Qwen2.5-32B 2.68 1.56 0.00
GSM8K-100 - Parsing Failure (%)
Llama3.1-8B (0.00 47.00 10.00
Gemma2-9B = 2.55 33.00 0.00
Qwen2.5-7B (0.00 40.00 0.00
Qwen?2.5-14B 0.00 16.00 0.00
Phi-4 (14B) 0.00 0.00 0.00
Qwen2.5-32B 0.00 0.00 0.00
27


--- Page 28 ---

Table 13: Gap in model performance depending on the choice of projector model. Reported numbers are calculated
by taking the difference of the max and min of the accuracies over all of the open-source projector models. Judge
results vary drastically depending on what model is being used as a projector. Smaller is better.
Projector Judge

ARC Challenge - A Accuracy (%)

Llama3.1-8B 0.07 0.18

Gemma2-9B 0.08 0.32

Qwen?2.5-7B 0.07 0.16

Qwen?2.5-14B 0.07 0.12

Phi-4 (14B) 0.07 0.08

Qwen?2.5-32B 0.07 0.10

GPQA Diamond - A Accuracy (%)

Llama3.1-8B 0.04 0.12

Gemma2-9B 0.07 0.24

Qwen?2.5-7B 0.08 0.12

Qwen?2.5-14B 0.10 0.19

Phi-4 (14B) 0.07 0.15

Qwen?2.5-32B 0.08 0.18

GPQA Main - A Accuracy (%)

Llama3.1-8B 0.05 0.10

Gemma2-9B 0.04 0.27

Qwen?2.5-7B 0.03 0.13

Qwen?2.5-14B 0.04 0.20

Phi-4 (14B) 0.06 0.17

Qwen?2.5-32B 0.05 0.18

28
