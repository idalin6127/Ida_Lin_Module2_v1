

--- Page 1 ---

Type of manuscript: Original research article
Title: A chart review process aided by natural language processing and multi-wave adaptive
sampling to expedite validation of code-based algorithms for large database studies
Contributors: Shirley V Wang', Georg Hahn’, Sushama Kattinakere Sreedhara', Mufaddal Ma-
hesri', Haritha S. Pillai’, Rajendra Aldis', Joyce Lii', Sarah K. Dutcher”, Rhoda Eniafe?, Jamal
T. Jones”, Keewan Kim?, Jiwei He?, Hana Lee”, Sengwee Toh’, Rishi J Desai", Jie Yang"
Affiliations:
' Division of Pharmacoepidemiology and Pharmacoeconomics, Department of Medicine,
Brigham and Women’s Hospital, Harvard Medical School, Boston, MA
? Center for Drug Evaluation and Research, Food and Drug Administration, Silver Springs, MD
3 Department of Population Medicine, Harvard Medical School and Harvard Pilgrim Health Care
Institute, Boston, MA
*Co-senior authors
Corresponding author:
Shirley V Wang
1620 Tremont St Suite 3030
Boston, MA 02120
Email: swang1@bwh.harvard.edu
Phone: 617-525-8376
Conflict of Interest: SVW has been an ad hoc consultant to Exponent Inc, Cytel Inc, and MI-
TRE, a federally funded research and development center for the Centers for Medicare and
Medicaid Services. ST is a methods consultant for Pfizer, Inc. Other authors do not have any
conflicts of interest.
Source of funding: This project was supported by Task Order 75F0123F 19010 under Master
Agreement 75F40119D10037 from the US Food and Drug Administration (FDA). FDA coauthors
reviewed the study protocol, statistical analysis plan, and the manuscript for scientific accuracy
and clarity of presentation. Representatives of the FDA reviewed a draft of the manuscript for
presence of confidential information and accuracy regarding statement of any FDA policy. The
views expressed are those of the authors and not necessarily those of the US FDA.
Data and computing code availability: This methods study did not evaluate a hypothesis and
therefore was not pre-registered. The study protocol with logged amendments, open-source
SAS code in the CIDA query package used to extract the cohort of patients with obesity, and in-
put data with accompanying R code used to calculate performance metrics and prepare figures
are available in the project repository on the Open Science Framework: https://osf.io/3fe25/. The
open-source CORA tool used for NLP aided chart review is available
https://github.com/jiesutd/CORA. Our data use agreements prohibit sharing of source data and
data derivatives with non-covered individuals and organizations. However, Medicare and Medi-
caid data may be requested through ResDAC: https://resdac.org/.

1


--- Page 2 ---

Abstract:
Background: One of the ways to enhance analyses conducted with large claims databases is
by validating the measurement characteristics of code-based algorithms used to identify health
outcomes or other key study parameters of interest. These metrics can be used in quantitative
bias analyses to assess the robustness of results for an inferential study given potential bias
from outcome misclassification. However, extensive time and resource allocation are typically
required to create reference-standard labels through manual chart review of free-text notes from
linked electronic health records.
Methods: We describe an expedited process that introduces efficiency in a validation study us-
ing two distinct mechanisms: 1) use of natural language processing (NLP) to reduce time spent
by human reviewers to review each chart, and 2) a multi-wave adaptive sampling approach with
pre-defined criteria to stop the validation study once performance characteristics are identified
with sufficient precision. We illustrate this process in a case study that validates the perfor-
mance of a claims-based outcome algorithm for intentional self-harm in patients with obesity.
Results: We empirically demonstrate that the NLP-assisted annotation process reduced the
time spent on review per chart by 40% and use of the pre-defined stopping rule with multi-wave
samples would have prevented review of 77% of patient charts with limited compromise to pre-
cision in derived measurement characteristics.
Conclusion: This approach could facilitate more routine validation of code-based algorithms
used to define key study parameters, ultimately enhancing understanding of the reliability of
findings derived from database studies.
Keywords: chart review, Bayesian, multi-wave sampling, NLP, validation study

2


--- Page 3 ---

Background
Studies that use large databases containing patient health information collected as part of rou-
tine care (e.g. claims, electronic health records [EHR]) are increasingly being used to generate
real-world evidence to inform healthcare decision-making.'* However, such data are collected
for purposes other than research. Because of this, database studies often rely on clinical code-
based algorithms to identify key study parameters, such as the health outcome of interest. The
ability of such algorithms to accurately capture the clinical concept of interest can affect the va-
lidity of the research findings.° Therefore, it is important to conduct chart validation studies, first
to understand an algorithm’s measurement characteristics (e.g., positive predictive value [PPV])
by comparing against a reference standard, and second to use the measurement characteristics
in quantitative bias analyses that evaluate the potential impact of bias due to misclassification.
Accurately identifying and characterizing patients who may or may not have a particular clinical
condition (e.g., who experience a health outcome of interest) typically requires a chart validation
study with detailed manual review of clinical notes to establish reference-standard labels in vali-
dation studies. The substantial human effort required for this step makes the process of con-
ducting validation studies time-consuming and resource-intensive, ultimately hampering scala-
bility. In certain applications, such as post-marketing safety surveillance of newly marketed
medications, efficient completion of validation studies is a critical step to assess whether the
data source being considered is fit-for-use.®
To that end, our objective was to propose a process that has the potential to substantially expe-
dite the conduct of validation studies by leveraging Natural Language Processing (NLP)”® and
multi-wave adaptive sampling? "° methods. NLP techniques can expedite chart review by direct-
ing the attention of chart annotators to relevant clinical information highlighted in EHR free-text
notes. Multi-wave adaptive sampling techniques that identify sequential samples of charts for
manual review across strata of a population allow investigators to estimate performance

3


--- Page 4 ---

characteristics other than PPV, while also setting predefined stopping boundaries for success
and futility in terms of algorithm performance for a primary metric of interest, thus limiting the
number of charts that need to be reviewed. By reducing the time and resources required for cre-
ating reference standard labels, incorporating these methods into an expedited chart review pro-
cess could allow for more frequent and systematic validation of algorithms in large database
studies, ultimately enhancing understanding of the reliability of findings derived from database
studies.
Methods
There are seven iterative steps in the proposed chart review process flow chart (Figure 1). We
describe the methodology for this process and illustrate the implementation via a case study fo-
cused on validation of a claims-based algorithm for intentional self-harm in adult patients with
obesity. A study protocol with logged amendments is available in e-supplemental materials 1.
Case study: Validation of claims-based algorithm for intentional self-harm in patients
with obesity
Suicidal outcomes are an important drug safety concern that clinical trials are generally not pow-
ered or designed to detect. Large healthcare database studies can generate complementary ev-
idence on safety endpoints; however, accurate assessment of suicidal outcomes — including
suicidality, suicidal ideation, attempted suicide, and intentional self-harm — are challenging to
define and accurately capture in administrative claims data. A systematic review identified 34
validation studies for algorithms to capture suicidal outcomes and classified them into two types
of algorithms. The first type included “screening” algorithms with higher sensitivity and lower
specificity that were intended to be followed up with chart validation. The second type included
“outcome definition” algorithms with more specific and restrictive criteria resulting in higher PPV
that would be considered adequate to be used for a study endpoint. '' Screening algorithms

4


--- Page 5 ---

often included diagnosis codes for probable/possible cases such as poisoning or overdose with-
out restrictions to specific healthcare settings. These screening algorithms generally had PPVs
less than 60% whereas the outcome definition algorithms that had the highest PPVs (ranging
from 74% to 92%) incorporated information on emergency department or hospital stays with
specific combinations of diagnostic codes. Generally, the sensitivity, specificity, and negative
predictive value (NPV) of the algorithms were infrequently reported.
For our case study, we validated a screening algorithm for intentional self-harm that included
International Classification of Diseases, 10 Revision, Clinical Modification (ICD-10-CM) codes
observed in any care setting and any diagnosis position (e-supp/emental materials 2) in a study
population of adult patients with obesity. This population is at higher risk for intentional self-harm
than the general population. 12%
Data Source
Mass General Brigham (MGB) is one of six sites in the Sentinel Real World Evidence Data En-
terprise (RWE-DE) that is frequently leveraged by the FDA for methodological and regulatory
studies of interest.'* MGB has linked Medicare claims data (2007-2020) and Medicaid claims
(2000-2018) deterministically to EHRs for patients receiving care at the MGB healthcare system
(linkage success rate 99.9% for Medicare and 99.3% for Medicaid). The claims data include de-
mographics, enrollment start and end dates, outpatient dispensed medications, medical diagno-
ses and performed procedures across care settings including inpatient, outpatient, skilled nurs-
ing and rehabilitation claims. These are linked with clinical assessment files, including the mini-
mum data set (MDS) of a comprehensive assessment of all residents in Medicare or Medicaid
certified facilities, the Outcome and Assessment Information Set (OASIS), a patient assessment
tool used in Medicare home health care to plan care, determine reimbursement, and measure
quality, and the Inpatient Rehabilitation Facility Patient Assessment Instrument (IRF-PAI) which
is used to collect patient assessment data for quality measure calculation. MGB is the largest

5


--- Page 6 ---

healthcare provider system in Massachusetts, comprising >40 healthcare facilities across the
full care continuum. The EHR databases contain information on patient demographics, medical
diagnosis and procedures, medications, vital signs, smoking status, body mass index (BMI), im-
munizations, laboratory data, clinical notes, and reports. MGB investigators have direct access
to full-text clinical notes for these patients, including ambulatory notes, discharge summaries,
and specialty reports (e.g., psychiatry, pulmonary). The linked claims-EHR dataset includes a
total of 1.2 million patients; of whom nearly 40% (~484,000) have at least one healthcare en-
counter in the most recent year. Finally, these patients have been deterministically linked to
state vital statistic files from Massachusetts, Connecticut, and Vermont to track long term out-
comes of all-cause and cause-specific mortality.
Study Population
The study population in which we validated the algorithm included patients over 18 years of age
with obesity who were enrolled in Medicare or Medicaid, received care at MGB. The cohort en-
try date was the first date for which a patient had a relevant obesity diagnosis code and also
met the eligibility criteria. Key eligibility criteria included a requirement for baseline observability
within claims data (continuous enrollment for the 180 days prior to and including the cohort entry
date) as well as EHR data (at least one visit within 180 days and at least one visit more than
180 days prior to the index date) and no evidence of self-harm diagnosis codes within the 180
days prior to and including the index date (additional eligibility criteria described in Figure $1,
Table $1). Patients were allowed to enter the cohort after October 1°‘, 2016, in order to ensure
that the baseline assessment windows were entirely within the era after the United States
switched from ICD-9-CM to ICD-10-CM. Patients were followed until they had an intentional
self-harm outcome, death, disenrollment from claims, or end of available data. The SAS based
query package using the Sentinel Cohort Identification and Descriptive Analysis Tool (CIDA)"®
including code lists are available in e-supplemental materials 3.

6


--- Page 7 ---

Chart Review Process

1. Create annotation guide with objective operational criteria
First, an annotation guide with objective criteria for defining the outcome of interest, for in-
tentional self-harm, was created. The annotation guide is available in e-supplemental mate-
rial 4. Annotators were instructed to look for positive assertions of suicide attempt or inten-
tional self-harm in EHR clinical notes that referred to the time frame of 30 days prior to or on
the date of relevant diagnosis. Notes after the date of relevant diagnosis could be used if the
notes referred to an event that occurred within 30 days prior to or on the date of relevant di-
agnosis. Whenever available, the annotators relied on the psychological or psychiatric as-
sessment of the clinical team treating the patient, for example, whether a 1:1 sit-in was or-
dered or there was a referral for psychiatric care.

2. Classify endpoint as occurring (y = 1) or not (y = 0) during follow-up according to the claims
algorithm
We used the CIDA query package and claims-based ICD-10-CM screening algorithm for in-
tentional self-harm to identify outcomes for the cohort of patients with obesity.

3. Create strata for sampling
Our sampling strata were defined by whether there is evidence to support a possible inten-
tional self-harm event from claims data and/or EHR data as shown in Figure 2. Sampling
within such strata and then applying inverse sampling weights (described in step 6) allows
estimation of measures of performance other than PPV, such as NPV, sensitivity, and speci-
ficity.°
To create these strata, we first used the claims-based ICD-10-CM screening algorithm to
classify patients as claims positive for having an intentional self-harm event (claims+) or

7


--- Page 8 ---

claims negative (claims-) during follow up. We then used MetaMap“® to filter through EHR
notes for all patients in the cohort. We identified a list of concept unique identifiers (CUI) re-
lated to self-harm by searching the UMLS meta-thesaurus
(https://uts.nim.nih.gov/uts/umls/home). Our search terms included: “self-harm”, “suicide”.
The list of CUls (1725 for self-harm and 967 for suicide, 2,681 unique CUls after removing
duplicates from the intersection of the two lists, e-supp/emental materials 5). Using the iden-
tified CUls, patients with no notes containing any such CUI within their follow up were con-
sidered EHR negative (EHR-), meaning that we considered these patients to have no evi-
dence to support occurrence of intentional self-harm. Those with at least one mention of
such CUI in their notes, suggesting a possible occurrence of intentional self-harm, were con-
sidered EHR positive (EHR+) for the purpose of creating strata.
In Figure 2, patients who were both claims- and EHR- (group 1) were not sampled for anno-
tation. Instead, the reference standard label was assumed to be negative for intentional self-
harm. Patients with cause of death in state death records coded as suicide (group 3) were
assumed to have experienced intentional self-harm and were also not sampled for annota-
tion; instead, the reference standard label was assumed to be positive for intentional self-
harm.
For patients who were either claims+ or [claims- and EHR+] (group 2), equal numbers of
claims+ and claims- patients were sampled in successive waves for annotation. For the
claims+ patients, this group was subdivided based on whether the patients had healthcare
contact at MGB within +/- 60 days of the claims-based algorithm outcome date (Figure 2
boxes d, e, f, and g). This subdivision was made because the annotators were instructed to
make their assessments based on the content of clinical notes. As annotators would not be
able to make assessments for patients not seen within MGB because of the absence of

8


--- Page 9 ---

clinical documentation, we could not conduct validation for this subsample (Figure 2, box d
and e). We made the simplifying assumption that the proportion of true positives out of all
claims+ was the same for patients seen at MGB as for patients seen outside the health sys-
tem. For patients who were claims- but EHR+ (Figure 2, boxes b and c), notes within +/- 60
days of the first note with a relevant CUI related to self-harm occurring within patient follow
up were reviewed for annotation.
4. Select a random sample batch of size k
In our example, there were 265 patients for whom the claims-based screening algorithm for
intentional self-harm was positive and had healthcare contact within the MGB healthcare
system around the date of the relevant claim (Figure 2 boxes f and g). We identified an
equal number of randomly sampled patients who were claims negative but had CUls related
to self-harm or suicide (Figure 2 boxes b and c).
We took sequential waves of random samples from the full set of 530 patient charts (265
claims+ and 265 claims-). In each wave, we took a new batch of size k = 10 for chart review
until the pool of samples was depleted. Each batch included five claims+ and five claims-
patient charts (a disproportionate stratified sample). Inverse sampling weights were used to
correct the sampling process. In parallel work, we have demonstrated that this approach
balances simplicity and efficiency compared to more complex sampling strategies in a multi-
wave setting, across a variety of scenarios.”
In order to compare our performance characteristics at the point of meeting a stopping crite-
rion to what the performance would have been had we continued reviewing, we completed
chart review on the entire sample of 530 charts.
5. Trained annotators review and label patients as endpoint occurring (Y = 1) or not (Y = 0)
9


--- Page 10 ---

We used Clinical Optimized Record Annotation (CORA), a time-contextualized NLP-assisted
EHR review tool developed in Python with PyQt5'®, which is compatible with Linux, Windows
and Mac operating systems. CORA loads both structured data and unstructured patient
notes, aggregates them, and displays them in chronological order within a graphical user in-
terface (GUI) (e-supplemental materials 6). |t highlights terms in unstructured notes based
on a user-customized keyword list to enhance manual review efficiency. To tailor CORA to
our endpoint of interest, we uploaded the previously identified 2,681 unique CUls related to
self-harm. Doing so allowed CORA to highlight text that matched the CUls and expedite the
chart review process for the annotators.
The pair of annotators had both medical training and master’s degrees in public health. They
were instructed to assign the intentional self-harm label based on the content of EHR notes
and in accordance with the criteria outlined in the annotation guide. There was a multi-step
process for training annotators. First, 30 charts were sampled for double annotation. Next,
inter-rater reliability was assessed using Cohen’s Kappa. '*?' As specified in our protocol, if
kappa >0.8 then we would move onto independent annotation of subsequent waves of sam-
pled batches. If kappa <0.8 then double annotation would continue, and the kappa was re-
calculated in sequential waves until it exceeded 0.8. Charts where the annotator was unsure
were discussed with a practicing board certified psychiatrist and resulting decisions were
used to update the annotation guide.
The time required for annotation of each patient chart was automatically recorded by CORA.
After training was completed with high kappa observed, the annotators were asked to inde-
pendently review sequential samples of batch of k = 10 charts loaded into CORA. In order to
compare annotation time with and without NLP assistance, each annotator additionally re-
viewed a sample of 20 charts without NLP highlighted terms that had previously been

10


--- Page 11 ---

reviewed by the other annotator with NLP highlighted terms. This provided us with a sample
of 40 charts with recorded timing of review both with and without NLP assistance.
6. Compute the quantity of interest and confidence bands

The primary measure of interest for the claims-based screening algorithm for intentional self-
harm was the PPV. However, based on the sampling strategy, we were also able to esti-
mate NPV, sensitivity, and specificity.
The PPV was estimated using all accumulated reference standard labels at each successive
wave of sampling. Bayesian credible intervals were used to quantify uncertainty, denoted as
p. Bayesian credible intervals were computed with a Beta-Binomial model, given by a
Beta(1,1) prior on the unobserved parameter p which is then updated once Binomial sam-
ples are observed. The Beta(1,1) prior represents our non-informative belief that the PPV
can range anywhere from 0 to 1. Upon observing more data, we update our model, leading
to a refinement of the PPV estimate. To be precise, after having observed s successes
(events) among k samples (reviewed charts, our knowledge of the PPV is described by a
Beta(1+s,1+k-s) posterior). To arrive at a credible interval, we compute the a/2-quantile qa/2
and the (1-a/2)-quantile q1-a2 of the Beta(1+s,1+k-s) posterior, which then form the credible
interval [Qa/2,q1-a/2]. Note that no alpha spending is required in the Bayesian case. The
Bayesian credible intervals are always computed with a=0.05.
When estimating the other measurement characteristics, due to unbalanced sampling frac-
tions from claims+ and claims- patients, we are not able to simply calculate the metrics
based on annotations of the sampled charts. Instead, we estimate the metrics after
weighting based on the inverse sampling fraction to reflect estimates of the metric as if the
whole cohort had been annotated. Groups 1 and 3 were considered fully annotated as being
truly negative or positive for intentional self-harm, respectively, therefore patients in these
groups have a sampling weight of 1.0. The weights for group 2 were determined by the

11


--- Page 12 ---

sampling fractions for claims+ versus claims- patients sampled for annotation at the time
that the stopping criterion for PPV was met. This analysis was performed using the statisti-
cal language R (details on confidence interval calculation provided in e-supplemental materi-
als 7, analytic code and data in e-supplemental data 1).
7. Iterate steps 5-6 until stopping criterion is met
Because our primary metric of interest was the PPV, our stopping rules focused on sequen-
tial evaluation of the PPV. Our stopping rule was defined by the lower bound of the Bayes-
ian credible interval for the PPV being above 0.75 (indicating high accuracy of the algorithm)
or the upper bound of the Bayesian credible interval being below 0.75 (indicating futility in
achieving an acceptably high performance).
Ethics review
This Sentinel project is a public health surveillance activity conducted under the authority of the
FDA and, accordingly, is not subject to Institutional Review Board (IRB) oversight. It received
exemption from Mass General Brigham IRB on their independent review.
Transparency and data sharing statement
This methods study did not evaluate a hypothesis and therefore was not pre-registered. The
study protocol with logged amendments, open-source SAS code in the CIDA query package
used to extract the cohort of patients with obesity, and input data with accompanying R code
used to calculate performance metrics and prepare figures are available in the project repository
on the Open Science Framework: https://osf.io/3fe25/.
The open-source CORA tool used for NLP aided chart review is available
https://github.com/jiesutd/CORA.
12


--- Page 13 ---

Our data use agreements prohibit sharing of source data and data derivatives with non-covered
individuals and organizations. However, Medicare and Medicaid data may be requested through
ResDAC: https://resdac.org/.
Results
There were 62,129 adult patients with obesity identified in the MGB EHR-claims linked data-
base. The average age was 63.4 years, 62.1% of the patients were female, and 76.8% were
white. The average BMI in this cohort was 36.3 kg/m2. The prevalence of selected comorbidi-
ties, baseline medications, and healthcare utilization are shown in Table 1.
As the kappa between the two annotators was 100% after 30 charts, subsequent batches were
independently annotated. The median time to review each patient chart using the NLP embed-
ded in the CORA tool to highlight relevant text was 7.0 minutes. This ranged from less than 0.1
minute to 92.6 minutes. The average number of notes available to review per patient was 45.7
and ranged from 1 to 512 notes. During annotator training, which included dual review of the
same 30 charts, the two annotators had similar timing in terms of review. When the trained an-
notators were asked to independently review the same charts, half without the use of NLP high-
lighted terms that the other annotator reviewed with NLP assistance, we observed that the

13


--- Page 14 ---

median time to review for this subset of charts was 6.0 minutes for NLP aided annotation com-
pared to 11.4 minutes for annotation without NLP assistance.
In randomly sampled sequential batches of k = 10 charts, the cumulative estimated PPV met
the stopping criterion for futility after 12 batches (23% of the full sample). At this point, the esti-
mated PPV was 0.60 with 95% credible interval (0.47, 0.72). As shown in Figure 3, the esti-
mated cumulative PPV stabilized around the time of the stopping criterion being met. Annotating
additional charts (total = 530) marginally tightened the credible intervals without substantive
changes to the point estimate. The inverse sampling weighted sensitivity, specificity, and NPV
are shown in Table 2 and indicated generally high performance that were consistent at the time
of meeting the stopping criterion and after review of the full sample, although confidence inter-
vals were wide for sensitivity due to the large weights within the small sample.
We characterized the reasons why the annotators classified cases as false positives in Table 3.
The most common reason was because the only text related to self-harm in the EHR notes
were mental status assessments which indicated no suicidal or homicidal thoughts or behavior
(28.6%). The next most common reason was that all of the text in the notes related to past his-
tory of suicide attempts or intentional self-harm, occurring more than 90 days prior to the rele-
vant claims code (18.4%). Many of these events were documented as occurring years prior to
the claims code. The third most common reason was due to patient drug overdose, where the
patient denied that the overdose was a suicide attempt, and the psychological evaluation and
subsequent treatment plans were aligned with this assessment of lack of intent to cause self-
harm (16.3%).
Twenty-three patients were classified as false negatives because they were completed suicides
identified by cause of death in linked state death records without recent healthcare contact that
resulted in a relevant claims diagnosis code or CUI in an EHR note (Figure 2). There was also
a false negative identified in the chart reviewed sample where there was evidence of a suicide
14


--- Page 15 ---

attempt in the EHR notes, resulting in psychiatric hospitalization; however, upon review of
claims diagnoses, only diagnosis codes related to suicidal ideation were found. Codes for sui-
cidal ideation were not included in the intentional self-harm claims code algorithm.
Discussion
In this paper, we introduce a chart review process that leverages two distinct enhancements 1)
NLP assisted review and 2) multi-wave adaptive sampling. The process is designed to expedite
validation of code-based algorithms for large database studies and support sensitivity analyses
that evaluate the potential impact of bias due to misclassification. In our illustrative case study
that validates a screening algorithm for intentional self-harm in patients with obesity, we ob-
served that using a stopping criterion for futility in the adaptive multi-wave samples could have
saved substantial time and resources by preventing unnecessary review of 77% of charts. In ad-
dition, NLP aided review with the open-source CORA tool reduced the amount of time to anno-
tate charts by 48% without compromising inter-annotator reliability.
The proposed process has many strengths. It can be flexibly applied across study populations
and algorithms. The sampling strategy based on strata defined by sources of evidence makes it
possible to estimate performance metrics other than PPV, such as sensitivity, specificity, and
NPV. Alternative stopping rules can be applied, depending on the needs and priorities of the in-
vestigators (e.g., width of credible interval, success and futility set at different levels). CORA is
open-source, lightweight, has an easy-to-use GUI, and is compatible with clinical notes ex-
tracted from different healthcare systems.
There are also limitations to the proposed process that merit discussion. First, we used a rela-
tively simple NLP approach based on MetaMap to both identify “true negatives” and identify
concepts to highlight in clinical notes. Although we cast a wide net to be sensitive in terms of rel-
evant CUls when filtering through notes, it is possible that patients whose notes do not include
15


--- Page 16 ---

language that match the CUls will be incorrectly classified as “true negatives”. Additionally,
while negation detection is conducted within MetaMap when searching for concepts, we should
acknowledge that it may not be fully accurate. Some negated concepts may be incorrectly rec-
ognized as present, some affirmatively-mentioned concepts may be missed, and that certain
words, phrasing, and clinical concepts may be missed or misclassified due to synonymy, the
complexity of human language, and/or the unique semantic and structural challenges posed by
clinical notes. As newer NLP approaches continue to develop, including large language models
(LLM), they can be plugged into the framework of the proposed process for more sophisticated
NLP-aided human review. However, in the context of chart review, the NLP is used only as an
aid to direct the attention of the human annotator to areas where there is likely to be relevant
text to review. That said, any NLP-assisted tool could have the effect of reducing annotator at-
tention to areas that are not highlighted. A limitation of the stratified sampling approach to facili-
tate estimation of performance characteristics is the necessary assumption that the proportion
of true positives is the same for the subset of patients who are seen within the healthcare sys-
tem to which claims data are linked as it is for patients who are seen in other healthcare sys-
tems. In general, when using success/futility stopping criteria with NLP-aided annotation and
multi-wave adaptive sampling, the greatest efficiency gains are anticipated for algorithms that
are clearly in the high validity range or in the futility range. For algorithms that are near the bor-
der, gains may be more limited.
There are also limitations to chart review that are not resolved by this process. The assumptions
that are used for quantitative bias sensitivity analyses are informed by the measurement charac-
teristics reported from chart review studies. These assumptions are more realistic and useful for
bias adjustment when measurement characteristics are available separately for the exposure
and comparator group of interest.?° In our illustrative example, we focused on a defined study
population, agnostic to exposures. This is in line with practical application of an expedited chart
16


--- Page 17 ---

review where the population and endpoint of interest are known, but either the exposure/com-
parator is not selected, the exposure/outcome combination is too rare, or the goal is to obtain a
more generic evaluation of performance that could be flexibly applied in quantitative bias anal-
yses across many potential exposures of interest.
Finally, self-harm and suicidal outcomes are challenging to define and different case definitions
may be appropriate for different goals. There is not a uniform set of terms or taxonomy for iden-
tifying this spectrum of ideation and/or behaviors.” Our case definition was based on identifying
intentional self-harm and inferring this intent based on the reactions of the treating clinical team
as documented in the EHR notes. The estimated PPV was low but in the same range or higher
than many other screening algorithms for intentional self-harm."' Alternative case definitions
would result in different performance metrics (example in e-supplemental appendix 8).
Conclusion
We have demonstrated a proposed process to expedite conduct of validation studies through
more efficient chart review. The time and resource savings from implementing this process
could facilitate more routine validation of outcome and other algorithms that are used to define
key study parameters and enhance understanding of the potential impact of bias due to misclas-
sification in database studies. Future work could add to the capabilities of this process by build-
ing in iterative tuning of claims-based algorithms to optimize performance for a given case defi-
nition/phenotype over multi-wave adaptive samples, rapidly testing alternative case definitions,
or incorporating LLMs that iteratively learn and enhance the usefulness of NLP aided highlights
in successive waves.

17


--- Page 18 ---

Tables and figures
Table 1. Characteristics of cohort of patients with obesityTable 2. Performance characteristics
at the time a stopping criterion was met versus after review of full sample
Table 3. Reasons for false positive and false negative annotation
Figure 1. Process diagram for natural language processing assisted chart review with adaptive,
multi-wave sampling
Figure 2. Counts in stratified sampling for estimation of performance metrics
Figure 3. Positive predictive value in cumulative multi-wave samples
Supplement
Figure S1. Design diagram for obesity cohort
Table $1. Eligibility flow for obesity cohort
Stored on OSF:
eSupplemental Materials 1. Study protocol with logged amendments
eSupplemental Materials 2. International Classification of Diseases 10‘ Revision code list for
intentional self-harm
eSupplemental Materials 3. SAS based query package using the Sentinel Cohort Identification
and Descriptive Analysis Tool (CIDA)
eSupplemental Materials 4. Annotation guide
eSupplemental Materials 5. Concept Unique Identifiers related to intentional self-harm or sui-
cide
18


--- Page 19 ---

eSupplemental Materials 6. Screenshot of graphical user interface (GUI) for Clinical Optimized
Record Annotation (CORA).
eSupplemental Materials 7. Calculation of weighted confidence intervals
eSupplemental Materials 8. Positive predictive value with alternative case definition
eSupplemental Data 1. De-identified annotation data and analytic code

19


--- Page 20 ---

Table 1. Characteristics of cohort of patients with obesity

pe MGB EHR-claims linked data (2016-2020)

SC

SC

SC

eee
20


--- Page 21 ---

So

So

So

es Se

i i pe
21


--- Page 22 ---

So
22


--- Page 23 ---

GLP-1 agonists prior use 1,663
GLP-1 agonists concurrent use 1,300
SGLT2 inhibitors prior use 632
SGLT2 inhibitors concurrent use 507 ee
DPP4 inhibitors prior use 1,383
DPP4 inhibitors concurrent use 1,124
EHR continuity measure 0.5
Low Income Subsidy 29,436
Mean number of ambulatory encounters 16.7
Mean number of emergency room encounters | 1
Mean number of inpatient hospital encounters | 0.5
Mean number of non-acute institutional en- 0.6 3.7
counters
Mean number of other ambulatory encounters | 2.5
Mean number of filled prescriptions 22.5
Mean number of generics dispensed 8.7

MGB: Mass General Brigham

EHR: electronic health record

GLP-1 agonist: Glucagon-like peptide-1 agonist

SGLT2 inhibitor: Sodium-glucose cotransporter 2 inhibitor

DPP4 inhibitors: Dipeptidyl peptidase-4 inhibitor

*Not shown per CMS small cell suppression policy

23


--- Page 24 ---

Table 2. Performance characteristics at the time a stopping criterion was met versus after
review of the full sample
When stopping criterion met After review of all charts
N = 120 N = 530
0.6034 (0.4744, 0.7193) 0.6343 (0.5751, 0.6897)
0.9996 (0.9849, 0.9996) 0.9988 (0.9948, 0.9995)
Sensitivity? | 0.9318 (0.2060, 0.9354) 0.8158 (0.4899, 0.9187)
Specificity? 0.9964 (0.9952, 0.9974) 0.9968 (0.9963, 0.9973)
PPV = positive predictive value
NPV = negative predictive value
1 PPV reflects cumulative estimates over multiple waves of sampling; Values in parentheses represent 95% credible
intervals
2 NPV, Sens, Spec are inverse sample weighted to the cohort size/distribution (sensitive to weights in small samples).
Values in parentheses represent 95% confidence intervals
24


--- Page 25 ---

Table 3 Reasons for false positive annotation
Total
(N = 98)
False Positives
No evidence of intentional self-harm or related concepts

The only text related to suicidal outcomes comes from mental status assess-
ment 28 28.6%

e.g. “no suicidal or homicidal behavior or thoughts”

No EHR notes referencing events within the 30 days prior to the date of rele- 3
vant claims diagnosis _
History of concepts related to intentional self-harm: ideation, family his- ef
tory, and history of drug overdose or alcohol withdrawal

Past history of suicidal ideation

History of drug overdose or alcohol withdrawal
Timing of intentional self-harm outside of case definition |} 00%

Past history of suicide attempt/intentional self-harm >90 days prior to rele- 0

, ; 18 18.4%
vant claims diagnosis

Recent history of suicide attempt/intentional self-harm within 31-90 days
prior to relevant claims diagnosis “

Suicide attempt/intentional self-harm AFTER relevant claims diagnosis
Evidence of suicidal ideation without evidence of self-harm Pf

Active suicidal ideation

Passive suicidal ideation 12.2%
Evidence of current overdose without intent to self-harm | 00%

Current overdose but patient denies suicide attempt/intentional self-harm,
psychological/psychiatric evaluation 16 16.3%

and clinical follow up consistent with this assessment

25


--- Page 26 ---

Figure 1. Process diagram for natural language processing assisted chart review with
adaptive, multi-wave sampling
1. Create annotati = with
objective operational criteria for
endpoint
2 y en as occurring
(y=1) or not (y=0) according to
claims data in follow up after time 0
$ based on sources
of evidence for endpoint
‘ m patient batch of Select the next batch of size k
size k according to sampling strategy
NLP tool highlights z batch and
relevant text in clinical labels endpoint as occurring (Y=1) no
notes or not (Y¥;=0)
and updated yes . |
confidence intervals ANS ; ! Stop. Report validated claims _
teri 4 | algorithm performance characteristics
T(y) = measurement characteristic of interest
y = claims algorithm classification of the outcome (1 = outcome occurred, 0 = outcome did not occur)
Y = reference standard label for the outcome (1 = outcome occurred, 0 = outcome did not occur)
The NLP tool used in our example was Clinical Optimized Record Annotation (CORA):
https://github.com/jiesutd/CORA
26


--- Page 27 ---

Figure 2. Counts in stratified sampling for estimation of performance metrics
Obese patient
No evidence of self- sone EXRCROTEe Death w/ cause of
harm in claims or EHR 47308 Prcesaaans Eek cSeeEa Ce 26 death suicide
EHR notes
Claim-based
self-harm
algorithm
i 2 | Ba +! +
« 4 = cc ce
z= x= =
“ “ wo + a
E E E E E
5 - 270 «8 265 5 5
= BS
338 s
= So
265 28 ge
2 32
3s ae
Manual chart review in vs that proportion of Manual chart review in
suialanmeden of dime true positives are the same for salad enriches of aims i
sequential samples of size care outside of the EHR system sequential samples of size
a. True b. True c. False d. True e. False f. True g. False h. True i. False
Negative Negative Negative Positive Positive Positive Positive Positive Negative
| }\ I |
y a, rs
Group 1 Group 2 Group 3
Boxes d and e are not reviewable due to lack of patient contact with the healthcare system where there is linked elec-
tronic health records to claims data.
27


--- Page 28 ---

Figure 3. Positive predictive value in cumulative multi-wave samples

o

©

oO

©

Oo
>
ou
ou

wT

o

N

Oo

id

oO

0 10 20 30 40 50
batch number
Dashed horizontal line is at the stopping criteria. “Success” stopping criterion = lower bound of credible interval ex-
ceeds 0.75. “Futility” stopping criterion = upper bound of credible interval below 0.75. Vertical solid line represents the
point at which the stopping criterion was met. Each batch included 10 patient charts with at least 1 relevant claims
diagnosis or CUI in the EHR notes.
28


--- Page 29 ---

Figure S1. Design diagram for obesity cohort
Cohort Entry Date (CED, Day 0)
Date of meeting obesity criterion
INCL: Continuous claims enrollment
(30 days gap allowed)
Days [-180, 0]
INCL: EHR observable time
1 visit within 180 days prior or on day 0 and 1 visit at least 181
days prior to day 0
[all available, 0]
EXCL: self harm
EXCL: Age <18, missing sex
Days [0,0] ——
Covariate Assessment Window
Days [-180, 0]
Y
Yy Follow Up Window Yy
Days [1, censor*] YY
VW
———_—_—_—_—_—_—_—_————————————————————————————————————————————————————— __________sSs=e
Time
Day 0
*Censoring criteria
° Disenrollment
. Death
° End of data
29


--- Page 30 ---

Table $1. Eligibility flow for obesity cohort
Eligibility criteria Obesity cohort
Output cohort from Query Request Package | 134,019
(QRP)
Exclude if patients do not have at least 1 66,838
EHR visit in baseline (180 days)
Exclude if patients do not have at least 1 62,129
EHR visit prior to baseline
Final obesity cohort 62,129
30


--- Page 31 ---

References
1. U.S. Food and Drug Administration: Framework for FDA's Real-World Evidence Pro-
gram. https:/Awww.fda.gov/media/120060/download
2. ICH. General Principles on Plan, Design and Analysis of Pharmacoepidemiological Stud-
ies That Utilize Real-World Data for Safety Assessment of Medicines. https://data-
base.ich.org/sites/default/files/ICH M14 Step3 DraftGuideline 2024 0521.pdf
3. Ball R, Robb M, Anderson SA, Dal Pan G. The FDA's sentinel initiative--A comprehen-
sive approach to medical product surveillance. Clin Pharmacol Ther. Mar 2016;99(3):265-8.
doi:10.1002/cpt.320
4. CMS. Proposed Guidance Document: Study Protocols That Use Real-world Data.
https://www.cms.gov/medicare-coverage-database/view/medicare-coverage-docu-
ment.aspx?mcdid=39
5. Lash TL, Fox MP, Cooney D, Lu Y, Forshee RA. Quantitative Bias Analysis in Regula-
tory Settings - PubMed. American journal of public health. 2016
Jul;106(7)doi:10.2105/AJPH.2016.303199
6. Desai RJ, Wang SV, Sreedhara SK, et al. Process guide for inferential studies using
healthcare data from routine clinical practice to evaluate causal effects of drugs (PRINCIPLED):
considerations from the FDA Sentinel Innovation Center. BMJ. 2024-02-12 2024:e076460.
doi:10.1136/bmj-2023-076460
7. Joshi A. Natural language processing - PubMed. Science (New York, NY).
09/13/1991 ;253(5025)doi:10.1126/science.253.5025.1242
8. Carrell DS, Halgrim S, Tran D-T, et al. Using natural language processing to improve ef-
ficiency of manual chart abstraction in research: the case of breast cancer recurrence - Pub-
Med. American journal of epidemiology. 03/15/2014;179(6)doi:10.1093/aje/kwt441
9. Collin LU, MacLehose RF, Ahern TP, et al. Adaptive Validation Design: A Bayesian Ap-
proach to Validation Substudy Design With Prospective Data Collection - PubMed. Epidemiol-
ogy (Cambridge, Mass). 2020 Jul;31(4)doi:10.1097/EDE.0000000000001 209
10. Wright A, Pang J, Feblowitz JC, et al. A method and knowledge base for automated in-
ference of patient problems from structured data in an electronic medical record - PubMed.
Journal of the American Medical Informatics Association : JAMIA. 2011 Nov-
Dec;18(6)doi:10.1136/amiajnI-2011-000121
11. Swain RS, Taylor LG, Braver ER, Liu W, Pinheiro SP, Mosholder AD. A systematic re-
view of validated suicide outcome classification in observational studies - PubMed. /nternational
journal of epidemiology. 10/01/2019;48(5)doi:10.1093/ije/dyz038
12. Wagner B, Klinitzke G, Brahler E, Kersting A. Extreme obesity is associated with suicidal
behavior and suicide attempts in adults: results of a population-based representative sample -
PubMed. Depression and anxiety. 2013 Oct;30(10)doi:10.1002/da.22105
13. Mather AA, Cox BJ, Enns MW, Sareen J. Associations of obesity with psychiatric disor-
ders and suicidal behaviors in a nationally representative sample - PubMed. Journal of psycho-
somatic research. 2009 Apr;66(4)doi:10.1016/j.josychores.2008.09.008
14. Desai Rishi J, Marsolo K, Smith J, et al. The FDA Sentinel Real World Evidence Data
Enterprise (RWE-DE). Pharmacoepidemiology and Drug Safety.
2024/10/01 ;33(10)doi:10.1002/pds. 70028
15. Sentinel Routine Querying System Overview. Accessed December 17, 2024, Browse
Sentinel Documentation / Sentinel Routine Querying Tool Documentation - Sentinel Version
Control System
16. Aronson AR, Lang F-M. An overview of MetaMap: historical perspective and recent ad-
vances - PubMed. Journal of the American Medical Informatics Association : JAMIA. 2010 May-
Jun;17(3)doi:10.1136/jamia.2009.002733

31


--- Page 32 ---

17. Hahn G, Schneeweiss S, Wang S. Adaptive multi-wave sampling for efficient chart vali-
dation. 2025/03/08;doi:10.48550/arXiv.2503.06308
18. Limited RC. PyQt5 - Comprehensive Python Bindings for Qt v5. https://pypi.org/pro-
ject/PyQt5/
19. Cohen J. A Coefficient of Agreement for Nominal Scales. Educational and Psychological
Measurement. 1960-04;20(1 )doi:10.1177/001316446002000104
20. Cohen J. Weighted Chi Square: an Extension of the Kappa Method. Educational and
Psychological Measurement. 1972-04;32(1)doi:10.1177/001316447203200106
21. Cohen J. Weighted kappa: Nominal scale agreement provision for scaled disagreement
or partial credit. Psychological Bulletin. 1968;70(4)doi:10.1037/h0026256
22. Basic HHS Policy for Protection of Human Research Subjects, 45 CFR §46.102(I)(2).
Accessed April 14, 2025, 2025. https://www.ecfr.gov/current/title-45/subtitle-A/subchapter-
A/part-46/subpart-A#46.102
23. Newcomer SR, Xu S, Kulldorff M, Daley MF, Fireman B, Glanz JM. A primer on quantita-
tive bias analysis with positive predictive values in research using electronic health data. Journal
of the American Medical Informatics Association : JAMIA. 2019 Jul 31;26(12)doi:10.1093/ja-
mia/ocz094
24. Silverman MM. Challenges to Classifying Suicidal Ideations, Communications, and Be-
haviours. /nternational Handbook of Suicide Prevention. 201 1;doi:10.1002/9781119998556.ch1
25. Lanes S, Beachler DC. Validation to correct for outcome misclassification bias. Phar-
macoepidemiology and Drug Safety. 2023/06/01 ;32(6)doi:10.1002/pds.5601
26. Applying Quantitative Bias Analysis to Epidemiologic Data. doi:10.1007/978-3-030-
82673-4
27. Fox MP, Lash TL, Bodnar LM. Common misconceptions about validation studies - Pub-
Med. International journal of epidemiology. 08/01/2020;49(4)doi:10.1093/ije/dyaa090
28. Brenner H, Gefeller O. Use of the positive predictive value to correct for disease misclas-
sification in epidemiologic studies - PubMed. American journal of epidemiology.
12/01/1993;138(11)doi:10.1093/oxfordjournals.aje.a1 16805

32
